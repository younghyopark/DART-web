<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>DexHub and DART</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css">
    <link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.28.0/themes/prism.min.css" rel="stylesheet" />
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.28.0/prism.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.28.0/components/prism-bash.min.js"></script>
    
    <style>
        body {
            font-family: 'Helvetica Neue', Arial, sans-serif;
            line-height: 1.6;
            color: #333;
            max-width: 800px;
            margin: 0 auto;
            padding: 20px;
            background-color: #f9f9f9;
        }
        h1, h2, h3 {
            font-weight: 300;
            color: #2c3e50;
        }
        h1 {
            font-size: 2.5em;
            margin-bottom: 0.5em;
            text-align: center;
            line-height: 1.2;
        }
        h2 {
            font-size: 1.8em;
            margin-top: 1.5em;
        }
        p {
            margin-bottom: 1.5em;
            text-align: justify;
        }
        .authors {
            font-style: italic;
            color: #7f8c8d;
            margin-bottom: 0.5em;
            text-align: center;
            font-size: 1.1em;
        }
        .authors span {
            display: inline-block;
            margin: 0 10px;
        }
        .paper-info {
            text-align: center;
            color: #2980b9;
            font-size: 1.1em;
            margin-bottom: 1em;
            font-weight: 500;
        }
        .info-container {
            display: flex;
            justify-content: space-between;
            align-items: center;
            margin-bottom: 1em;
        }
        .conference-info {
            background-color: #e0f7fa;
            padding: 8px 15px;
            border-radius: 5px;
            color: #01579b;
            font-size: 0.9em;
            flex: 0 1 auto;
        }
        .conference-info strong {
            font-weight: bold;
        }
        .links {
            display: flex;
            gap: 15px;
            flex: 0 1 auto;
        }
        .links a {
            color: #3498db;
            text-decoration: none;
            display: inline-flex;
            align-items: center;
            font-size: 0.9em;
        }
        .links a:hover {
            text-decoration: underline;
        }
        .links img {
            width: 16px;
            height: 16px;
            margin-right: 5px;
        }
        .abstract {
            background-color: #fdfdfd;
            padding: 3px 20px 20px 20px; /* Reduced top padding */
            border-radius: 5px;
            box-shadow: 0 2px 15px rgba(0,0,0,0.1);
            margin-bottom: 2em;
        }
        .content-section {
            background-color: #fff;
            padding: 3px 20px 20px 20px; /* Reduced top padding */
            border-radius: 5px;
            box-shadow: 0 2px 15px rgba(0,0,0,0.1);
            margin-bottom: 2em;
        }
        .figure {
            background-color: #e0e0e0;
            height: 300px;
            display: flex;
            justify-content: center;
            align-items: center;
            margin: 1em 0;
            font-size: 1.2em;
            color: #7f8c8d;
        }
        .figure-container {
          margin: 2em 0;
          text-align: center;
        }
        .figure-container img {
            max-height: 300px;
            max-width: 100%;
            height: auto;
            /* border: 1px solid #ddd; */
            /* border-radius: 4px; */
            /* box-shadow: 0 2px 4px rgba(0,0,0,0.1); */
        }
        .figure-container2 {
          margin: 2em 0;
          text-align: center;
        }
        .figure-container2 img {
            max-height: 500px;
            max-width: 100%;
            height: auto;
            /* border: 1px solid #ddd; */
            /* border-radius: 4px; */
            /* box-shadow: 0 2px 4px rgba(0,0,0,0.1); */
        }

        .figure-caption {
            margin-top: 0.5em;
            /* font-style: italic; */
            color: #666;
            font-size: 0.9em;
        }
        @media (max-width: 600px) {
            body {
                padding: 10px;
            }
            h1 {
                font-size: 2em;
            }
            .authors span {
                display: block;
                margin: 5px 0;
            }
            .info-container {
                flex-direction: column;
                align-items: stretch;
            }
            .conference-info, .links {
                width: 100%;
                text-align: center;
                margin-bottom: 10px;
            }
            .links {
                justify-content: center;
            }
        }
        .quote {
            border-left: 4px solid #ccc; /* Line on the left */
            padding-left: 16px; /* Indentation */
            margin-left: 16px;
            color: #555; /* Text color */
            font-style: italic; /* Optional: Italicize text */
        }
        .custom-link {
            color: #3498db; /* Custom color for the link */
            text-decoration: none; /* Remove default underline */
        }

        .custom-link:hover {
            color: #2980b9; /* Slightly different color on hover */
            text-decoration: underline; /* Optional: underline on hover */
        }

        .acronym-explanation {
        font-style: italic; /* Italicize text */
        font-size: 0.9em; /* Slightly smaller font */
        color: #555; /* A lighter color for the text */
        text-align: left; /* Align text to the left */
        margin-top: 0px; /* Add a little spacing above */
        }

        .video-container {
            margin: 20px 0;
        }

        .video-controls {
            display: flex;
            justify-content: center;
            gap: 10px;
        }

        .video-controls button {
            padding: 5px 10px; /* Adjust these values to control the padding inside the buttons */
            font-size: 14px;   /* Optionally, reduce font size for smaller buttons */
            cursor: pointer;
        }

        .video-controls .active-button {
            background-color: blue;
            color: white;
        }

        #videoPlayer {
            width: 100%;  /* Adjust the width as needed */
            height: 280px;
            object-fit: cover;  /* Ensures the video covers the area */
            overflow: hidden;   /* Hides the cropped portions */
            padding: 0; 
        }

        .cropped-video {
        object-fit: cover; /* Crops the video */
        width: 70%;
        height: auto;
        /* You can further adjust the cropping using these properties */
        object-position: center; /* Center the video inside the container */
        clip-path: inset(0 1% 0 1%); /* top right bottom left */
        }

        .cropped-video2 {
        object-fit: cover; /* Crops the video */
        width: 100%;
        height: auto;
        /* You can further adjust the cropping using these properties */
        object-position: center; /* Center the video inside the container */
        clip-path: inset(0 1% 0 1%); /* top right bottom left */
        }


        .vertical-cropped-video {
        object-fit: cover; /* Crops the video */
        width: 70%;
        height: auto;
        /* You can further adjust the cropping using these properties */
        object-position: center; /* Center the video inside the container */
        clip-path: inset(5% 0% 5% 0%); /* top right bottom left */
        }


        .new_video {
            border-radius: 15px; /* Adjust the value to control the roundness */
            overflow: hidden; /* Ensures content stays within the rounded borders */
            box-shadow: 0 4px 8px rgba(0, 0, 0, 0.2); /* Optional: Add some shadow for a subtle effect */
        }


        .callout {
            padding: 1em;
            margin: 1em 0;
            border-left: 4px solid #f1c40f; /* Yellow color like a lightbulb */
            background-color: #f9f9f9; /* Light background */
            /* font-size: 1em; */
            display: flex;
            align-items: center;
        }
        .callout a {
            white-space: nowrap; /* Prevents the link from breaking to a new line */
            text-decoration: underline;
        }

        .callout2 {
            background-color: #fdf6e3; /* Soft background color */
            border-left: 4px solid #e2b007; /* Accent border on the left */
            padding: 16px; /* Ample padding for readability */
            margin: 16px 0; /* Vertical spacing */
            border-radius: 8px; /* Rounded corners */
            font-size: 1.1rem; /* Slightly larger font for emphasis */
            color: #333; /* Dark text color for contrast */
            max-width: 700px; /* Optional: Constrain width for readability */
            box-shadow: 0 2px 8px rgba(0, 0, 0, 0.1); /* Soft shadow for depth */
        }

        .callout2 a {
            color: #007acc; /* Distinct color for the link */
            font-weight: bold;
            text-decoration: none;
        }

        .callout2 a:hover {
            text-decoration: underline; /* Underline on hover for link indication */
        }

        .callout2-icon {
            font-weight: bold;
            color: #e2b007;
            font-size: 1.3rem;
            margin-right: 8px;
            vertical-align: middle;
        }


        /* .callout::before {
            content: "💡";
            font-size: 1.5em;
            margin-right: 0.5em;
        } */
        .logo-container {
            display: flex;
            justify-content: center;
            align-items: center;
            gap: 10px; /* Adjust the spacing between logos */
            margin-top: 10px;
        }

        .logo {
            display: inline-block;
        }

        .mit-logo {
            height: 50px;
        }

        .lab-logo {
            height: 40px;
        }

        .copy-link {
            font-size: 0.9em;
            color: #7f8c8d; /* Gray color */
            cursor: pointer;
            margin-left: 10px;
            display: inline-flex;
            align-items: center;
        }
        .copy-link i {
            font-size: 1.2em; /* Adjust size if needed */
            margin-right: 5px;
        }

        .toc-nav {
            position: fixed;
            right: 20px;
            top: 50%;
            transform: translateY(-50%);
            background: rgba(255, 255, 255, 0.95);
            padding: 15px;
            border-radius: 8px;
            box-shadow: 0 2px 10px rgba(0, 0, 0, 0.1);
            max-width: 300px;
            max-height: 80vh;
            overflow-y: auto;
            z-index: 1000;
            font-size: 14px;
        }

        .toc-nav h3 {
            margin: 0 0 10px 0;
            padding-bottom: 8px;
            border-bottom: 1px solid #eee;
            font-size: 16px;
            color: #333;
        }

        .toc-nav ul {
            list-style: none;
            padding: 0;
            margin: 0;
        }

        .toc-nav li {
            margin: 8px 0;
            padding-left: 10px;
            border-left: 2px solid transparent;
            transition: border-color 0.2s ease;
        }

        .toc-nav a {
            color: #666;
            text-decoration: none;
            display: block;
            padding: 4px 8px;
            border-radius: 4px;
            transition: all 0.2s ease;
        }

        .toc-nav a:hover {
            color: #2980b9;
            background: #f5f5f5;
        }

        .toc-nav li.active {
            border-left: 2px solid #2980b9;
        }

        .toc-nav li.active a {
            color: #2980b9;
            font-weight: 500;
        }

        @media (max-width: 1200px) {
            .toc-nav {
                display: none;
            }
        }

        .target-demographic {
            display: inline-block;
            font-size: 0.85em;
            color: #666;
            background-color: #f5f5f5;
            padding: 2px 8px;
            border-radius: 4px;
            margin-left: 8px;
            border-left: 3px solid #2980b9;
        }


        .code-container {
            background-color: #f4f4f4; /* Light gray background */
            border-radius: 8px; /* Rounded corners */
            padding: 12px; /* Slightly smaller padding */
            margin: 8px 0; /* Reduced vertical spacing above and below the container */
            overflow-x: auto; /* Allows horizontal scrolling for long lines */
            font-family: 'Courier New', Courier, monospace; /* Monospaced font */
            font-size: 1rem; /* Font size */
            color: #333; /* Darker text color for better readability */
        }

        .code-container code {
            color: #333; /* Matching the text color */
            font-size: 0.95em; /* Slightly smaller text within code */
            line-height: 1.5; /* Improved readability */
        }

        .footnote-link {
            color: #0066cc;
            cursor: pointer;
            font-size: 0.75em;
            vertical-align: super;
            text-decoration: none;
            margin: 0 2px;
        }

        .footnote-modal {
            display: none;
            position: fixed;
            top: 50%;
            left: 50%;
            transform: translate(-50%, -50%);
            background-color: white;
            padding: 20px;
            border-radius: 8px;
            box-shadow: 0 2px 10px rgba(0,0,0,0.1);
            z-index: 1001;
            max-width: 80%;
            width: auto;
            max-height: 80vh;
            overflow-y: auto;
        }

        /* New styles for grouped footnotes */
        .footnote-group {
            margin-bottom: 15px;
            padding-bottom: 15px;
            border-bottom: 1px solid #eee;
        }

        .footnote-group:last-child {
            border-bottom: none;
            margin-bottom: 0;
            padding-bottom: 0;
        }

        .footnote-number {
            font-weight: bold;
            color: #0066cc;
            margin-right: 10px;
        }

        /* Rest of previous styles */
        .footnote-overlay {
            display: none;
            position: fixed;
            top: 0;
            left: 0;
            right: 0;
            bottom: 0;
            background-color: rgba(0,0,0,0.5);
            z-index: 1000;
        }

        .footnotes-section {
            margin-top: 40px;
            padding-top: 20px;
            border-top: 2px solid #eee;
        }

        .footnote-content {
            display: none;
        }

        .close-modal {
            position: absolute;
            right: 10px;
            top: 10px;
            cursor: pointer;
            font-size: 20px;
            color: #666;
        }

        .video-row {
            display: flex;
            gap: 20px; /* Space between videos */
            justify-content: center; /* Center videos horizontally */
        }
        
        .figure-container-side-by-side {
            width: 45%; /* Adjust to control individual video width */
        }


        
    </style>

<div class="footnote-overlay" id="footnoteOverlay"></div>
<div class="footnote-modal" id="footnoteModal">
    <span class="close-modal">&times;</span>
    <div id="footnoteContent"></div>
</div>

<script>


    document.addEventListener('DOMContentLoaded', function() {
        // Handle all internal links with hash
        document.querySelectorAll('a[href^="#"]').forEach(anchor => {
            anchor.addEventListener('click', function (e) {
                e.preventDefault();
                const targetId = this.getAttribute('href').slice(1);
                const targetElement = document.getElementById(targetId);
                
                if (targetElement) {
                    targetElement.scrollIntoView({
                        behavior: 'smooth',
                        block: 'start'
                    });
                    
                    // Update URL without jumping
                    window.history.pushState(null, null, `#${targetId}`);
                }
            });
        });

        // Enhance footnote scrolling
        document.querySelectorAll('.footnote-link').forEach(link => {
            link.addEventListener('click', function(e) {
                // First handle the modal display as before
                const footnoteNumber = this.textContent.replace(/[\[\]]/g, '');
                const footnoteContent = document.querySelector(`#footnote-${footnoteNumber}`);
                
                if (footnoteContent) {
                    const modal = document.getElementById('footnoteModal');
                    const content = document.getElementById('footnoteContent');
                    content.innerHTML = `
                        <div class="footnote-group">
                            <span class="footnote-number">[${footnoteNumber}]</span>
                            ${footnoteContent.innerHTML}
                        </div>
                    `;
                    modal.style.display = 'block';
                    document.getElementById('footnoteOverlay').style.display = 'block';
                    
                    // Smooth scroll to the modal
                    modal.scrollIntoView({
                        behavior: 'smooth',
                        block: 'center'
                    });
                }
            });
        });

        // Handle initial load with hash
        if (window.location.hash) {
            const targetElement = document.querySelector(window.location.hash);
            if (targetElement) {
                // Small delay to ensure proper scrolling after page load
                setTimeout(() => {
                    targetElement.scrollIntoView({
                        behavior: 'smooth',
                        block: 'start'
                    });
                }, 100);
            }
        }
    });

    // Update all footnote content elements to have proper IDs
    document.querySelectorAll('.footnote-content').forEach((content, index) => {
        content.id = `footnote-${index + 1}`;
    });


    document.addEventListener('DOMContentLoaded', function() {
        const footnoteElements = document.querySelectorAll('.footnote-content');
        const footnotesList = document.getElementById('footnotesList');
        
        // Process footnotes and identify groups
        let currentGroup = [];
        let allGroups = [];
        let lastNode = null;
        
        footnoteElements.forEach((elem, index) => {
            if (lastNode && areConsecutive(lastNode, elem)) {
                currentGroup.push({ elem, index });
            } else {
                if (currentGroup.length > 0) {
                    allGroups.push([...currentGroup]);
                }
                currentGroup = [{ elem, index }];
            }
            lastNode = elem;
        });
        if (currentGroup.length > 0) {
            allGroups.push(currentGroup);
        }
        
        // Process groups and individual footnotes
        allGroups.forEach(group => {
            if (group.length > 1) {
                // Create grouped footnote
                const startIndex = group[0].index + 1;
                const endIndex = group[group.length - 1].index + 1;
                const link = document.createElement('a');
                link.className = 'footnote-link';
                link.textContent = `[${startIndex}-${endIndex}]`;
                link.onclick = () => showGroupedFootnotes(group);
                
                // Insert single link before first footnote
                group[0].elem.parentNode.insertBefore(link, group[0].elem);
                
                // Add to footnotes list
                group.forEach(({ elem, index }) => {
                    addToFootnotesList(elem, index + 1);
                });
                
                // Remove other footnotes in group
                group.slice(1).forEach(({ elem }) => {
                    if (elem.previousSibling && elem.previousSibling.className === 'footnote-link') {
                        elem.previousSibling.remove();
                    }
                });
            } else {
                // Process individual footnote
                const elem = group[0].elem;
                const index = group[0].index;
                
                const link = document.createElement('a');
                link.className = 'footnote-link';
                link.textContent = `[${index + 1}]`;
                link.onclick = () => showFootnote(elem.innerHTML, index + 1);
                
                elem.parentNode.insertBefore(link, elem);
                addToFootnotesList(elem, index + 1);
            }
        });
    });

    function areConsecutive(node1, node2) {
    // Get the actual text content between two nodes
        let textBetween = '';
        let current = node1;
        
        while (current && current !== node2) {
            // Skip footnote content nodes
            if (!current.classList || !current.classList.contains('footnote-content')) {
                if (current.nodeType === Node.TEXT_NODE) {
                    textBetween += current.textContent;
                } else if (current.nodeType === Node.ELEMENT_NODE) {
                    textBetween += current.textContent;
                }
            }
            current = current.nextSibling;
        }
        
        // Clean up the text (remove whitespace, newlines, etc.)
        textBetween = textBetween.trim();
        
        // Check if there's any significant text between footnotes
        // Also check if they're in the same block-level element
        return textBetween.length < 3 && // Allow only very short separators like commas
            node1.parentElement === node2.parentElement && // Must be in same parent
            !hasBlockElementBetween(node1, node2);
    }

    function hasBlockElementBetween(node1, node2) {
        const blockElements = ['DIV', 'P', 'H1', 'H2', 'H3', 'H4', 'H5', 'H6', 'SECTION'];
        let current = node1;
        
        while (current && current !== node2) {
            if (current.nodeType === Node.ELEMENT_NODE && 
                blockElements.includes(current.tagName)) {
                return true;
            }
            current = current.nextSibling;
        }
        
        return false;
    }

    function showGroupedFootnotes(group) {
        const modal = document.getElementById('footnoteModal');
        const content = document.getElementById('footnoteContent');
        
        content.innerHTML = group.map(({ elem, index }) => `
            <div class="footnote-group">
                <span class="footnote-number">[${index + 1}]</span>
                ${elem.innerHTML}
            </div>
        `).join('');
        
        modal.style.display = 'block';
        document.getElementById('footnoteOverlay').style.display = 'block';
    }

    function showFootnote(content, index) {
        const modal = document.getElementById('footnoteModal');
        const footnoteContent = document.getElementById('footnoteContent');
        
        footnoteContent.innerHTML = `
            <div class="footnote-group">
                <span class="footnote-number">[${index}]</span>
                ${content}
            </div>
        `;
        
        modal.style.display = 'block';
        document.getElementById('footnoteOverlay').style.display = 'block';
    }

    function addToFootnotesList(elem, index) {
        const footnotesList = document.getElementById('footnotesList');
        const footnoteItem = document.createElement('div');
        footnoteItem.className = 'footnote-item';
        footnoteItem.innerHTML = `
            <span class="footnote-number">[${index}]</span>
            ${elem.innerHTML}
        `;
        footnotesList.appendChild(footnoteItem);
    }

    // Close modal handlers
    document.querySelector('.close-modal').onclick = () => {
        document.getElementById('footnoteModal').style.display = 'none';
        document.getElementById('footnoteOverlay').style.display = 'none';
    };

    document.getElementById('footnoteOverlay').onclick = () => {
        document.getElementById('footnoteModal').style.display = 'none';
        document.getElementById('footnoteOverlay').style.display = 'none';
    };

    document.addEventListener('DOMContentLoaded', function() {
        // Create TOC container
        const toc = document.createElement('nav');
        toc.className = 'toc-nav';
        toc.innerHTML = '<h3>Contents</h3><ul></ul>';
        document.body.appendChild(toc);
        
        const tocList = toc.querySelector('ul');
        const sections = document.querySelectorAll('.content-section');
        
        // Ensure all sections have IDs
        sections.forEach((section, index) => {
            if (!section.id) {
                section.id = `section-${index + 1}`;
            }
        });
        
        // Create TOC entries
        sections.forEach(section => {
            const tocTitle = section.getAttribute('data-toc-title');
            if (tocTitle) { // Only create TOC entry if section has a data-toc-title
                const li = document.createElement('li');
                const link = document.createElement('a');
                link.textContent = tocTitle;
                link.href = `#${section.id}`;
                
                link.addEventListener('click', (e) => {
                    e.preventDefault();
                    section.scrollIntoView({ behavior: 'smooth' });
                    window.history.pushState(null, null, link.href);
                });
                
                li.appendChild(link);
                tocList.appendChild(li);
            }
        });

        // Update active section on scroll
        function updateActiveSection() {
            let currentSection = '';
            
            sections.forEach(section => {
                const sectionTop = section.offsetTop;
                const sectionHeight = section.clientHeight;
                const scrollPosition = window.scrollY;
                
                // Consider a section active if we've scrolled past its top
                // and haven't reached the bottom of the section
                if (scrollPosition >= sectionTop - 100 && 
                    scrollPosition < sectionTop + sectionHeight - 100) {
                    currentSection = section.id;
                }
            });
            
            // Update active states in TOC
            tocList.querySelectorAll('li').forEach(li => {
                const link = li.querySelector('a');
                const href = link.getAttribute('href').substring(1); // Remove #
                
                if (href === currentSection) {
                    li.classList.add('active');
                    // Ensure active item is visible in scrollable TOC
                    li.scrollIntoView({ behavior: 'smooth', block: 'nearest' });
                } else {
                    li.classList.remove('active');
                }
            });
        }

        // Initial update
        updateActiveSection();
        
        // Update on scroll with throttling
        let ticking = false;
        document.addEventListener('scroll', () => {
            if (!ticking) {
                window.requestAnimationFrame(() => {
                    updateActiveSection();
                    ticking = false;
                });
                ticking = true;
            }
        });
    });


    document.addEventListener('DOMContentLoaded', function() {
        // Get all h2 elements within content sections
        document.querySelectorAll('.content-section h2').forEach((header, index) => {
            // Ensure each .content-section has a unique id
            const section = header.closest('.content-section');
            if (!section.id) {
                section.id = `section-${index + 1}`;
            }

            // Create a copy link button
            const copyLinkBtn = document.createElement('span');
            copyLinkBtn.classList.add('copy-link');
            copyLinkBtn.innerHTML = '<i class="fas fa-link"></i>';
            copyLinkBtn.setAttribute('title', 'Copy link to section');
            copyLinkBtn.style.marginLeft = '10px';
            copyLinkBtn.style.fontSize = '0.6em';
            copyLinkBtn.style.opacity = '0.7';
            copyLinkBtn.style.cursor = 'pointer';
            copyLinkBtn.style.verticalAlign = 'middle';

            // Add hover effect
            copyLinkBtn.addEventListener('mouseenter', () => {
                copyLinkBtn.style.opacity = '1';
            });
            copyLinkBtn.addEventListener('mouseleave', () => {
                copyLinkBtn.style.opacity = '0.7';
            });

            // Add click handler to copy the URL
            copyLinkBtn.addEventListener('click', (e) => {
                e.preventDefault();
                const url = `${window.location.origin}${window.location.pathname}#${section.id}`;
                
                // Copy to clipboard
                navigator.clipboard.writeText(url)
                    .then(() => {
                        // Show success message
                        const originalTitle = copyLinkBtn.getAttribute('title');
                        copyLinkBtn.setAttribute('title', 'Link copied!');
                        setTimeout(() => {
                            copyLinkBtn.setAttribute('title', originalTitle);
                        }, 2000);
                    })
                    .catch(err => {
                        console.error('Failed to copy:', err);
                    });
            });

            // Add the button next to the header
            header.appendChild(copyLinkBtn);
        });
    });
    var videos = [
        "static/videos/user_study_white_task1 1_25.00.mp4",
        "static/videos/user_study_white_task2 1_25.00.mp4",
        "static/videos/user_study_white_task3 1_25.00.mp4" // Add more video sources if needed
    ];

    // Function to load a video based on index
    function loadVideo(index) {
            const videoPlayer = document.getElementById('videoPlayer');
            
            if (index >= 0 && index < videos.length) {
                // Store current time and playing state
                const wasPlaying = !videoPlayer.paused;
                
                // Update video source
                videoPlayer.src = videos[index];
                
                // After loading new source, play if it was playing before
                videoPlayer.load();
                if (wasPlaying) {
                    videoPlayer.play();
                }

                // Update button styles
                const buttons = document.querySelectorAll('.video-controls button');
                buttons.forEach((button, i) => {
                    if (i === index) {
                        button.classList.add('active-button');
                    } else {
                        button.classList.remove('active-button');
                    }
                });
            }
        }

        // Initialize first video on page load
        window.onload = function() {
            loadVideo(0);
        };

        // Function to toggle content visibility
    function toggleContent(id) {
        var content = document.getElementById(id);
        if (content.style.display === "none") {
            content.style.display = "block";
        } else {
            content.style.display = "none";
        }
    };

</script>

</head>
<body>


    <h1>DexHub and DART: Infrastructure for <br>Internet-Scale Robot Data Collection</h1>
    <p class="authors">
        <span>Younghyo Park</span>
        <span>Jagdeep Bhatia</span>
        <span>Lars Ankile</span>
        <span>Pulkit Agrawal</span>
    </p>
    <!-- put MIT logo with fixed width  -->
    <div class="logo-container">
        <img src="static/images/mit_logo_std_rgb_mit-red.png" alt="MIT Logo" class="logo mit-logo">
        <img src="static/images/lab_logo.png" alt="Lab Logo" class="logo lab-logo">
    </div>
    
    <p class="paper-info">ICRA 2025 (Under Review)</p>

    <div class="info-container">
        <div class="conference-info">
            Visit <strong>CoRL 2024 XE/WBCM Workshop</strong> <br>for an in-person demo, Nov 9th, Munich
        </div>
        <div class="links">
            <a href="https://arxiv.org/abs/2407.16186" target="_blank">
                <img src="static/images/papericon.png" alt="Paper icon"> Full Paper
            </a>
            <a href="https://dexhub.ai/" target="_blank">
                <img src="https://api.iconify.design/material-symbols:open-in-new.svg" alt="Portal icon"> DexHub Portal
            </a>
            <a href="https://x.com/younghyo_park/status/1815986042098348163" target="_blank">
              <img src="https://abs.twimg.com/favicons/twitter.ico" alt="Twitter icon"> Twitter
            </a>
          </div>
    </div>

    <!-- division line -->
    <hr style="border: 0; border-top: 1px solid #333; margin: 20px 0;">
    
    <div class="figure-container" style="margin:0%">
        <video width="70%" playsinline autoplay muted>
            <!-- <source src="static/videos/twitter_main_fig.mp4" type="video/mp4"> -->
             <!-- make it auto-loop, with no video control exposed -->
            <source src="static/videos/dart_logo_animated2.mp4" type="video/mp4">
        </video>  
        <caption style="margin:0%">
            <center>
            <b>Try it yourself on Apple Vision Pro!</b> Please see our <a href="#how-to-dart" class="custom-link">getting started guide</a>.
            </center>
        </caption>
        <!-- <img src="static/images/rl_wish.png" alt="What we wish RL could do"> -->
        <!-- <p class="figure-caption">A truly intelligent robot should be able to learn a new task without human supervision, removing the bottleneck to scaling capabilities.</p> -->
    </div>




    <!-- <center>
<iframe width="100%" height="400" src="https://www.youtube.com/embed/wLfRDlVcOWM?si=9d4W9JXMOCx_KN6N" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>        
</center>   -->
    <p> </p>




    <!-- <div class="abstract">
        <h2>Abstract</h2>
    
        <p>The field of robotics has long grappled with a critical challenge: the scarcity of diverse, high-quality data that can be used to train a generalist robot policy. 
            While real-world data collection efforts exist<span class="footnote-content">A. O’Neill, A. Rehman, A. Maddukuri, A. Gupta, A. Padalkar,
                A. Lee, A. Pooley, A. Gupta, A. Mandlekar, A. Jain et al., “Open
                x-embodiment: Robotic learning datasets and rt-x models: Open x-
                embodiment collaboration 0,” in 2024 IEEE International Conference
                on Robotics and Automation (ICRA). IEEE, 2024, pp. 6892–6903.</span><span class="footnote-content">H.-S. Fang, H. Fang, Z. Tang, J. Liu, C. Wang, J. Wang, H. Zhu, and
                C. Lu, “Rh20t: A comprehensive robotic dataset for learning diverse
                skills in one-shot,” in 2024 IEEE International Conference on Robotics
                and Automation (ICRA). IEEE, 2024, pp. 653–660.</span><span class="footnote-content">F. Ebert, Y. Yang, K. Schmeckpeper, B. Bucher, G. Georgakis,
                K. Daniilidis, C. Finn, and S. Levine, “Bridge data: Boosting generalization of robotic skills with cross-domain datasets,” arXiv preprint
                arXiv:2109.13396, 2021.</span><span class="footnote-content">A. Khazatsky, K. Pertsch, S. Nair, A. Balakrishna, S. Dasari, S. Karamcheti, S. Nasiriany, M. K. Srirama, L. Y. Chen, K. Ellis et al., “Droid:
                A large-scale in-the-wild robot manipulation dataset,” arXiv preprint arXiv:2403.12945, 2024.</span>,
            requirements for robot hardware, physical environment setups, and frequent resets significantly impede the scalability needed for modern learning frameworks. To address these limitations, this paper introduces DART, a novel teleoperation platform that reimagines robotic data collection by leveraging cloud-based simulation and augmented reality (AR). Our user studies highlight that DART enables higher data collection throughput and lower physical fatigue compared to real-world teleoperation frameworks. In addition, our policy training experiments using DART-collected datasets demonstrate successful Sim2Real transfer with robust trained behaviors. Most importantly, all data collected through DART is automatically stored in our cloud-hosted database, DexHub, and publicly available to anyone. DexHub also supports easy-to-use data logging APIs that can be inserted into anyone's robot execution script, positioning it as an ever-growing data hub for robot learning.
        </p>
    </div>
     -->

    <!-- <div class="content-section">
    <h2>Key Claims</h2>
      <ol>
        <li>We need to <b>automate</b> the heuristic process of Environment Shaping. </li>
        <li>We need <b>better RL algorithms</b> that don’t require heuristic environment shaping in the first place.</li>
        <li>Developing better RL algorithms starts from <b>benchmarking on unshaped RL environments.</b></li>
    </ol>

    </div> -->


    <!-- <div class="content-section">
        <h2>What is a <b>robot dataset</b>, and why do we need them?</h2>

        <center><b>We want a ChatGPT moment to happen for robotics.</b> </center>
        
        Robotics has seen impressive progress with the advent of
        learning-based control. However, a major bottleneck is the
        lack of diverse and high-quality data for training robust and
        generalizable robot policies. Access to internet-scale robotics
        dataset that continually and rapidly grows with data coming from everywhere in the world will be ideal — just like how
        people easily upload language, images and videos on the
        internet. Despite recent efforts, we are not there yet.
        In this paper, we examine and address many key bottlenecks
        in achieving this dream.
    </div> -->



    <div class="content-section" data-toc-title="How demos are collected today" id="status-quo">
        <h2>Today, robot data-collection doesn't scale</h2>
        
        <div class="figure-container">
            <video class="new_video" width="100%" playsinline  autoplay loop muted controls>
                <!-- <source src="static/videos/twitter_main_fig.mp4" type="video/mp4"> -->
                 <!-- make it auto-loop, with no video control exposed -->
                <source src="static/videos/typical_real_world_data_collection 1_23.00.mp4" type="video/mp4">
            </video>  
            <!-- <img src="static/images/rl_wish.png" alt="What we wish RL could do"> -->
            <!-- <p class="figure-caption">A truly intelligent robot should be able to learn a new task without human supervision, removing the bottleneck to scaling capabilities.</p> -->
        </div>
    

        <!-- <button class="toggle-button" onclick="toggleContent('step3')">See the details</button>
        <div id="step3" class="toggle-content"> -->

            <!-- Create a list -->
            <ol>
                <li>
                    <b>Buying a robot</b> is <u>expensive</u>, limiting data collection to tech companies or research labs.
                </li>
                <li>
                    <b>Setting up environments</b> requires either constructing them or moving robots to sites of interest, both of which are <u>difficult and time-consuming</u>.
                </li>
                <li>
                    <b>Teleoperating the robot</b> suffers from visual occlusions, network delays, and a lack of tactile feedback, which can <u>slow down operators</u> and prevent them from performing dynamic or precise tasks.
                </li>
                <li>
                    <b>Resetting the environment</b> after every task completion is <u>time-consuming</u> and physically and mentally exhausting. Operators need to context-switch between robot control and environment setup, and ensure that resets are diverse.
                </li>
                <li>
                    <b>Repeating</b> the process is difficult for humans, who struggle to focus when performing repetitive jobs. Unfortunately, the number of required demonstrations scales with task complexity and the extent of required generalization.
                </li>
                <li>
                    <b>Post-processing collected data</b> often happens on a local machine or private cloud. Different data structures and conventions for data storage <u>make data sharing difficult</u>.
            </ol>

            <!-- <p>
                <b>Step 1. Buy a robot &nbsp;|&nbsp;</b> 
                Most data collection efforts require <u>expensive</u>, <u>physical robots</u>, limiting data collection to tech companies or research labs.
            </p>

            <p>
                <b>Step 2. Setup an environment &nbsp;|&nbsp;</b> Data collectors must either physically construct a fake environment around the robot or move the robot to an environment of interest. Both are <u>difficult and time-consuming</u>. 
                
            </p>


            <p>
                <b>Step 3. Teleoperate the robot &nbsp;|&nbsp;</b> Visual occlusions, network delays, and a lack of tactile feedback can <u>slow down operators</u> and prevent them from performing dynamic or precise tasks. 
            </p>

            <p>
                <b>Step 4. Reset the environment for every task completion &nbsp;|&nbsp;</b>
                Resetting is <u>time-consuming</u> and both physically and mentally exhausting. Operators need to context-switch between robot control and environment setup, and ensure that resets are diverse.

            </p>
            
            
            <p>
                <b>Step 5. Repeat &nbsp;|&nbsp;</b> Humans struggle to focus when performing repetitive jobs. Unfortunately, The number of required demonstrations scales with task complexity and the extent of required generalization. 
            </p>

            <p>
                <b>Step 6. Post-Process collected data &nbsp;|&nbsp;</b> Often, recorded data is stored on a local machine or private cloud. Different data structures and conventions for data storage <u>make data sharing difficult</u>.
            </p> -->
        <!-- </div> -->
            <center>
                <b>How do we make this process scale to the size of vision/language datasets?</b>            
            </center>


    </div>
    


    <div class="content-section" data-toc-title="DART reimagines data collection" id="dart-intro">
      <h2 style="margin-bottom: 5px;"><b>DART</b> reimagines robot data collection</h2>



      <div class="figure-container"   >
        <video class="new_video" width="100%" playsinline  autoplay loop muted controls>
            <!-- <source src="static/videos/twitter_main_fig.mp4" type="video/mp4"> -->
             <!-- make it auto-loop, with no video control exposed -->
            <source src="static/videos/new_teaser 1_25.00.mp4" type="video/mp4">
        </video>  
        <!-- <img src="static/images/rl_wish.png" alt="What we wish RL could do"> -->
        <!-- <p class="figure-caption">A truly intelligent robot should be able to learn a new task without human supervision, removing the bottleneck to scaling capabilities.</p> -->
    </div>


      <p>
        <b>Collect Data Anywhere in Augmented Reality &nbsp;|&nbsp;</b> 
        DART leverages cloud-based simulation and AR to provide a <u>scalable</u>, <u>intuitive</u>, and <u>cost-effective</u> solution for data collection accessible anywhere in the world. This eliminates the need for a physical robot and associated environment setup costs.

      </p>
  
      <p>
        <b>Access Data through <a href="https://dexhub.ai">DexHub.ai</a> &nbsp;|&nbsp;</b> 
        All data collected through DART is <u>automatically logged</u> to the cloud and <u>shared publicly</u> with researchers.
      </p>
      
    </div>
      
    <div class="content-section" data-toc-title="How easy is DART to use?"  id="dart-user-study">

      <h2>User Study: <b>DART</b> is intuitive and efficient</h2>

      <!-- <p>We bet even your mom could use it.</p> -->

      <!-- <div class="callout">
        Can my mom, for instance, use this to collect data for her robot?
        </div> -->


        <p>
            <!-- <b>User Study &nbsp;|&nbsp;&nbsp;</b>   -->
            We recruited twenty robotics novice participants to spend seven minutes collecting demonstrations with the <a href="https://www.rainbow-robotics.com/en_main">Rainbow RB-Y1's</a> and ALOHA's<span class="footnote-content">T. Z. Zhao, V. Kumar, S. Levine, and C. Finn, “Learning fine-grained
                bimanual manipulation with low-cost hardware,” arXiv preprint
                arXiv:2304.13705, 2023.</span>
                default “kinematic double”-based teleoperation interfaces.

                <!-- A total of 20 robotics-novice participants (without any expertise in robotics or prior experience in teleoperating robots) were asked to spend 7 minutes, collecting as many robot demonstrations as possible. 
                For real-world teleoperation, we used each robot's default teleoperation interface; both <a href="https://www.rainbow-robotics.com/en_main">Rainbow RB-Y1</a> and ALOHA<span class="footnote-content">T. Z. Zhao, V. Kumar, S. Levine, and C. Finn, “Learning fine-grained
                    bimanual manipulation with low-cost hardware,” arXiv preprint
                    arXiv:2304.13705, 2023.</span>
                comes with a kinematic double as their default teacher device. -->
        </p>
        <p>
            <!-- <b>Data Throughput Comparison &nbsp;|&nbsp;</b>             Participants were able to collect 2.5x more demonstrations using DART compared to real-world teleoperation. 
            Participants also reported lower physical fatigue and higher task completion satisfaction when using DART. 
            The results suggest that DART is an intuitive and efficient platform for robot data collection. -->

            <!-- <p>Participants collected <b>2.5x more demonstrations</b> using DART than real-world teleoperation and reported lower physical fatigue and higher task completion satisfaction.</p> -->

            <p><b>Participants collected 2.5x more demonstrations using DART than real-world teleoperation</p></b>

            <div class="video-row">
                <div class="figure-container">
                    <video class="cropped-video2" width="100%" playsinline autoplay muted>
                        <source src="static/videos/animated_user_study_graph.mp4" type="video/mp4">
                        Your browser does not support the video tag.
                    </video>
                </div>
            
                <div class="figure-container">
                    <video class="cropped-video2" width="100%" playsinline autoplay muted>
                        <source src="static/videos/user_study_qualitative_2.mp4" type="video/mp4">
                        Your browser does not support the video tag.
                    </video>
                </div>
            </div>


            <!-- <div class="figure-container">
                <video class="cropped-video" width="70%" playsinline  autoplay muted >
                  <source id="videoSource" src="static/videos/animated_user_study_graph.mp4" type="video/mp4">
                  Your browser does not support the video tag.
                </video>
            </div> -->
    
            

            <!-- Many participants attributed this considerable data throughput gap to a) physical fatigue during teleoperation and b) their inability to closely observe local contact interactions, 
            which hindered their ability to perform tasks effectively. This particular attribution becomes evident with controlled ablation studies. -->

        </p>


        <p>
            <!-- <b>Survey Results &nbsp;|&nbsp;</b>      We also asked the participants to provide qualitative feedback on their experience with DART, and existing VR-based simulation teleoperation frameworks.  -->

            Participants attributed this gap to a) <u>reduced physical fatigue</u>, b) <u>better visibility</u> of local contact interactions, and c) <u>easier resetting</u> of the environment. Please see our paper for controlled ablation studies.

            <!-- <div class="figure-container">
                <video class="cropped-video" width="70%" playsinline autoplay muted >
                  <source id="videoSource" src="static/videos/user_study_qualitative_2.mp4" type="video/mp4">
                  Your browser does not support the video tag.
                </video>
            </div> -->
    
        </p>

        <p>
            In particular, participants spend data-collection time <u>more productively</u> in DART than the real world.

            <!-- <b>How people spend time &nbsp;|&nbsp;</b>  We also analyzed the time spent on different tasks during the data collection process.
            The results show that participants spent significantly less time on environment setup and resetting tasks when using DART compared to real-world teleoperation. -->

            <div class="figure-container">
                <video class="video" width="75%" playsinline autoplay muted >
                  <source id="videoSource" src="static/videos/time_spent_animation-converted.mp4" type="video/mp4">
                  Your browser does not support the video tag.
                </video>
            </div>


        </p>


        <!-- <b>Side-by-side Comparison Videos &nbsp;|&nbsp;</b>  Side-by-side videos comparing the data collection process in DART versus the real-world.  -->
        <p>See side-by-side comparison videos below.</p>

        <div class="video-controls" style="margin-top: 20px;">
            <button onclick="loadVideo(0)">Object Sorting w/ Rainbow RB-Y1</button>
            <button onclick="loadVideo(1)">Precise Insertion w/ Rainbow RB-Y1</button>
            <button onclick="loadVideo(2)">Bolt Nut Sorting w/ ALOHA</button>
        </div>

        <!-- need to show four videos with 2x2 grid -->
        <div class="video-container">
            <video class="vertical-cropped-video" id="videoPlayer" width="100%" playsinline autoplay muted >
              <source id="videoSource" src="static/videos/user_study_white_task1 1_25.00.mp4" type="video/mp4">
              Your browser does not support the video tag.
            </video>
        </div>

    </div>

      
    <!-- </div> -->

    
    <div class="content-section" data-toc-title="DART's Features and UI" id="dart-features">
        <h2>DART's features</h2>


        <p>
          <b>Instant Task/Robot Switching &nbsp;|&nbsp;</b> Humans are known to lose focus when given a highly repetitive job<span class="footnote-content">Häusser, J. A., Schulz‐Hardt, S., Schultze, T., Tomaschek, A., & Mojzisch, A. (2014). Experimental evidence for the effects of task repetitiveness on mental strain and objective work performance. Journal of Organizational Behavior, 35(5), 705-721. <a href="https://onlinelibrary.wiley.com/doi/full/10.1002/job.1920">[link]</a></span>.
            Instead of making teleoperators to get bored with continuously repeating the same task over and over, DART allows them to switch to a different task or robot as needed.
          This is not possible in real-world data collection, since setting up a new task requires a lot of time and effort; 
          they should get the most juice out of the current environment setup before moving on to the next task or setup.

          <div class="figure-container">
            <video class="new_video" width="80%" playsinline  autoplay muted loop>
              <source id="videoSource" src="static/videos/instant_task_switching 1_26.00.mp4" type="video/mp4">
              Your browser does not support the video tag.
            </video>
        </div>


        </p>


        <p>
            <b>Ego/Exo-Viewpoint Switching &nbsp;|&nbsp;</b> Although the default option is to let the teleoperators be in an ego-viewpoint, i.e., letting users be a robot themselves, 
            we often find it useful to switch to an exo-viewpoint to observe the robot's motion from a different perspective. In DART, users can easily switch between ego and exo viewpoints; 
            be the robot yourself, or sit in front of the robot, all still being super close to the place where important contacts happen. 

            <div class="figure-container">
                <video class="new_video" width="80%" playsinline autoplay muted loop>
                  <source id="videoSource" src="static/videos/egoexo-switch 1_26.00.mp4" type="video/mp4">
                  Your browser does not support the video tag.
                </video>
            </div>

          </p>
  

        <p>
            <b>One-Click Environment Reset &nbsp;|&nbsp;</b> Upon task completion, users can reset the environment instantly to collect a new trajectory.
            This is not possible in real-world data collection; users have to manually reset the environment with their own hands for every task completion.

            <div class="figure-container">
                <video class="new_video" width="80%" playsinline autoplay muted loop>
                  <source id="videoSource" src="static/videos/random_resets.mp4" type="video/mp4">
                  Your browser does not support the video tag.
                </video>
            </div>
  
          </p>
  

        <p>
            <b>Mobile Manipulation &nbsp;|&nbsp;</b> Using the global-frame tracking capabilities of AR devices, 
            users can walk around in an AR environment teleoperating mobile manipulators to complete tasks that require navigation and manipulation.

            <div class="figure-container">
                <video class="new_video" width="60%" playsinline autoplay muted loop>
                    <source id="videoSource" src="static/videos/mobile_manip.mp4" type="video/mp4">
                  Your browser does not support the video tag.
                </video>
            </div>
    

        </p>
      
  
        <p>
            <b>Multi-Robot Support &nbsp;|&nbsp;</b> If you are not sure which robot is best suited for your task, you can try multiple robots in DART to complete the task.
            
        </p>

        <p>
            <b>Support of Custom Scenes and Robots &nbsp;|&nbsp;</b> If you have a custom robot or a scene that you want to collect data for,
            you can upload them through our website and start collecting data right away.<span class="footnote-content">Custom scene / robot description upload will be available Jan 2025.</span>
        </p>
      </div>


    <div class="content-section" data-toc-title="DART's design choices" id="dart-design">
      <h2>Design choices we made</h2>

      <p>
        <b>Visual Rendering happens locally on the AR device &nbsp;|&nbsp;</b> Compared to existing VR-based teleoperation approaches which send over the entire camera feeds or rendered images over the network
        <span class="footnote-content">A. Iyer, Z. Peng, Y. Dai, I. Guzey, S. Haldar, S. Chintala, and
            L. Pinto, “Open teach: A versatile teleoperation system for robotic
            manipulation,” arXiv preprint arXiv:2403.07870, 2024.</span><span class="footnote-content">X. Cheng, J. Li, S. Yang, G. Yang, and X. Wang, “Open-television:
                teleoperation with immersive active visual feedback,” arXiv preprint
                arXiv:2407.01512, 2024</span>, 
        DART off-loads compute intensive visual rendering processes to an edge AR device, 
        reducing latency without compromising the visual fidelity.
        <div class="video-container">
            <video width="100%" playsinline autoplay muted >
            <source id="videoSource" src="static/videos/dart_structure.mp4" type="video/mp4">
            Your browser does not support the video tag.
            </video>
            </div>
        </p>


        <p>
            <b>Fingertip-based Robot Controls &nbsp;|&nbsp;</b> 
            For a more intuitive user experience, instead of relying on external hand tracking or control devices, 
            we use our hands as the primary input device for robot control. For parallel jaw grippers, 
            users can control the gripper's opening and closing by pinching and releasing their fingers, while the end-effector SE(3) pose is specified by 4 keypoints on human hands. 
            <div class="figure-container" >
                <img src="static/images/fingertips.png" width="60%" alt="DART Fingertip Control">
            </div>
            For dexterous hands, full hand keypoints are used to control all joint angles. 


        </p>
    
    

    </div>


    <div class="content-section" data-toc-title="Power of Simulation Demos" id="power-of-simulation">
        <h2><b>Demonstrations in Simulation</b> offers a lot of benefits. </h2>
        If you are a robotics researcher who already has access to a physical robot, you might be wondering, 

        <div class="callout">
            Why should I care about collecting data in simulation when I can collect data in the real world?
        </div>
        We argue that collecting data in simulation offers several benefits over real-world data collection. 

        <p>
            <b>1. Data Augmentation &nbsp;|&nbsp;</b> In simulation, there are multiple privileged information that you can easily access, 
            such as the ground truth state of the robot, the ground truth state of the environment, and the ground truth state of the objects.
            This allows a randomziation and augmentation strategy that is not possible in the real-world. 

            

        </p>




        <p>
            <b>2. RL Finetuning &nbsp;|&nbsp;</b> One of the powerful benefit of simulation is the possibility of using RL to finetune the policy. 
            Simulation allows for the refinement and
            augmentation of human-collected (therefore possibly
            suboptimal) datasets through online reinforcement learning
            using massively parallelizable simulation environment.
            Such refinement can address the potential performance saturation often observed on policies trained only with supervised
            learning<span class="footnote-content">S. Ross and D. Bagnell, “Efficient reductions for imitation learning,”
                in Proceedings of the Thirteenth International Conference on Artificial
                Intelligence and Statistics, ser. Proceedings of Machine Learning
                Research, Y. W. Teh and M. Titterington, Eds., vol. 9. Chia Laguna
                Resort, Sardinia, Italy: PMLR, 13–15 May 2010, pp. 661–668.
                [Online]. <a href="https://proceedings.mlr.press/v9/ross10a.html">[link]</a></span>
                <span class="footnote-content">S. Ross, G. Gordon, and D. Bagnell, “A reduction of imitation learning
                    and structured prediction to no-regret online learning,” in Proceedings
                    of the fourteenth international conference on artificial intelligence and
                    statistics. JMLR Workshop and Conference Proceedings, 2011, pp.
                    627–635.</span>
                <span class="footnote-content">T. Z. Zhao, J. Tompson, D. Driess, P. Florence, S. K. S. Ghasemipour,
                    C. Finn, and A. Wahid, “Aloha unleashed: A simple recipe for robot
                    dexterity,” in 8th Annual Conference on Robot Learning.</span>
                    <span class="footnote-content">L. Ankile, A. Simeonov, I. Shenfeld, M. Torne, and P. Agrawal, “From
imitation to refinement–residual rl for precise visual assembly,” arXiv
preprint arXiv:2407.16677, 2024.
                    </span>.



            <div class="figure-container">
                <video class="cropped-video2" width="100%" playsinline autoplay muted >
                  <source id="videoSource" src="static/videos/bcvsrl 1_25.00.mp4" type="video/mp4">
                  Your browser does not support the video tag.
                </video>
            </div>

        </p>

      </div>
  

      <div class="content-section" data-toc-title="Experience DART yourself!" id="how-to-dart">
        <h2><b>Experience DART yourself!</b></h2>

        <div class="callout2">
            <center>
                Good news, you can experience DART yourself! <br>If you have an Apple Vision Pro, sign up <a href="https://dexhub.ai/">here</a> for access. 
            </center>
        </div>

        <div class="figure-container">
            <video class="new_video" width="100%" playsinline autoplay muted controls >
              <source id="videoSource" src="static/videos/dart_tutorial_new2 1_25.00.mp4" type="video/mp4">
              Your browser does not support the video tag.
            </video>
        </div>

        <p>
            <b>Cloud-hosted Physics Engine &nbsp;|&nbsp;</b> 

            We support cloud-hosted physics engine on the following regions. Anyone can spin up a cloud-hosted physics engine 
            in their nearest server through our portal and start collecting data.         <span class="target-demographic">Target Demographic: <b>General Public</b></span>

            <figure>
                <img src="static/images/aws-supported-regions.png" width="100%" alt="DART Fingertip Control">
            </figure>
        </p>

        <p>
            <b>Locally-hosted Physics Engine &nbsp;|&nbsp;</b> If you have the skill and compute resource to run the physics engine locally, 
            you can install and run the engine yourself. <span class="target-demographic">Target Demographic: <b>Robotics Researchers / Engineers</b></span>

            <div class="code-container">
                <pre><code class="language-bash">pip install dart_physics --upgrade   
export DEXHUB_API_TOKEN=...
python -m dart_physics.server</code> </pre>
            </div>

            To download the data later from the cloud, make sure you export the API token as an environment variable. You can generate and find the API token in our <a href="https://dexhub.ai">DexHub</a> portal.

        </p>

      </div>


      <div class="content-section" id="real-world-datasets" data-toc-title="Real-world Demos are still useful">
        <h2><b>Real-world datasets will still be useful</b></h2>

        We are not claiming that simulation-collected demos are the only useful source of robot data; 
        In fact, we believe that a few real-world demonstrations will be very useful to fine-tune the policy learned from simulation data, 
        especially for last-mile Sim2Real transfer process. 
        
        <div class="figure-container" >
            <img src="static/images/dataset_fraction.png" width="70%" alt="data-fraction">
        </div>


        We still believe, however, that the scale and diversity of robot foundation model's behavior will mostly come from simulation demos, 
        where real-world datasets will mostly serve as a regularizer preventing the policy from over-exploiting the artifacts of simulation and/or deviating too much from the real-world counterparts. 


    </div>

  



      <div class="content-section" id="future-plans" data-toc-title="Call for Research">
        <h2><b>Call for Research / Code-Works</b></h2>

        <p>
            DART and DexHub are still in the early stages of development, which still lacks a lot of features that can significantly improve the data collection process.
        </p>

        <h3><b>Generative Simulation Scene Modeling</b></h3>
        <p>
            Although teleoperating in simulation removes the need for physical environment setup, 
            it still requires a human to manually design the scene in simulation. This involves either finding the right 3D asset in the internet, scanning real-world objects, or manually CAD-ing the objects from scratch. 
            All these, although still easier than setting up a physical environment, are still time-consuming and requires a certain level of expertise.
            Thus, a method that can autonomously create <b>manipulatable</b> simulation scenes either from language descriptions or pictures/videos of real-world scenes can further
            bring down the cost of data collection and increase the diversity of the collected data. 
        </p>
        <p>
        Recent advances including GenSim<span class="footnote-content">Wang, L., Ling, Y., Yuan, Z., Shridhar, M., Bao, C., Qin, Y., ... & Wang, X. (2023). Gensim: Generating robotic simulation tasks via large language models. arXiv preprint arXiv:2310.01361.</span>,
        RoboGen<span class="footnote-content">Wang, Y., Xian, Z., Chen, F., Wang, T. H., Wang, Y., Fragkiadaki, K., ... & Gan, C. (2023). Robogen: Towards unleashing infinite data for automated robot learning via generative simulation. arXiv preprint arXiv:2311.01455.
        </span>, Gen2Sim<span class="footnote-content">Katara, Pushkal, Zhou Xian, and Katerina Fragkiadaki. "Gen2sim: Scaling up robot learning in simulation with generative models." 2024 IEEE International Conference on Robotics and Automation (ICRA). IEEE, 2024.
        </span>, RoboCasa<span class="footnote-content">Nasiriany, S., Maddukuri, A., Zhang, L., Parikh, A., Lo, A., Joshi, A., ... & Zhu, Y. (2024). RoboCasa: Large-Scale Simulation of Everyday Tasks for Generalist Robots. arXiv preprint arXiv:2406.02523.</span>, RialTo<span class="footnote-content">Torne, M., Simeonov, A., Li, Z., Chan, A., Chen, T., Gupta, A., & Agrawal, P. (2024). Reconciling reality through simulation: A real-to-sim-to-real approach for robust manipulation. arXiv preprint arXiv:2403.03949.
        </span> have shown promising results, 
        but still has a long way to generate truly diverse and realistic scenes resembling that of the real world.
        </p>

        <h3><b>Integration with Whole-Body Keypoint Tracking Controllers</b></h3>

        <p>
            DART supports controlling (a) fixed-based manipulators, (b) upper-limb of humanoid platforms, and (c) wheel-based mobile robots from keypoint tracking inputs using Differential IK formulation. 
            The main missing piece, however, is the whole-body controller that can reliably track the keypoints in global frame. Integrating recent works like HOVER<span class="footnote-content">He, T., Xiao, W., Lin, T., Luo, Z., Xu, Z., Jiang, Z., ... & Zhu, Y. (2024). HOVER: Versatile Neural Whole-Body Controller for Humanoid Robots. arXiv preprint arXiv:2410.21229.</span>
            that can support arbitrary combination of keypoints for whole-body control can be a promising direction to explore.
        </p>
        
        <h3><b>Training Robot Policies from Multiple Sources</b></h3>

        <p> 
            Since simulation demos are not the only source of datasets to train robots, 
            a more principled method that can combine data from multiple sources, potentially with different data modalities, 
            can be a promising direction to explore.
        </p>

        <h3><b>Data Curation for Robot Learning</b></h3>

        <p> 
            Since the data collected through a crowd-sourced platform will be highly diverse and noisy, 
            a principled way to curate the data to ensure the quality of the data is crucial.
            It is very much well known that OpenAI had to spend a significant amount of time and effort to curate the data for their vision-language model,
            and we believe that a similar effort will be required for robot data collection as well, as we enter the regime of larger-sccale, crowd-sourced robot data collection.
        </p>


    </div>


    <div class="content-section" id="future-imagined" data-toc-title="Future We Imagine">
        <h2><b>Future we Imagine</b></h2>

        <p>
        We hope this project inspires the robotics community to think about / make progress on a platform that ignites the <b>ecological growth</b> of robot datasets. 
        </p>

        <p>
        We believe that robot dataset with the size of vision and language datasets cannot come from project-level data collection efforts, led by 
        single research labs or tech companies. Instead, it requires a community-driven effort, where researchers from all over the world can contribute to the dataset, 
        just like how every image, texts, comments, articles being uploaded to the internet are a potential source of vision-language model training. 
        </p>

        <div class="figure-container">
            <img src="static/images/future.png" alt="">

        </div>

    





    </div>

    <div class="footnotes-section">
        <div id="footnotesList"></div>
    </div>
    


</body>
</html>
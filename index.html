<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>DexHub and DART</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css">
    <link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.28.0/themes/prism.min.css" rel="stylesheet" />
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.28.0/prism.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.28.0/components/prism-bash.min.js"></script>
    
    <style>
        body {
            font-family: 'Helvetica Neue', Arial, sans-serif;
            line-height: 1.6;
            color: #333;
            max-width: 800px;
            margin: 0 auto;
            padding: 20px;
            background-color: #f9f9f9;
        }
        h1, h2, h3 {
            font-weight: 300;
            color: #2c3e50;
        }
        h1 {
            font-size: 2.5em;
            margin-bottom: 0.5em;
            text-align: center;
            line-height: 1.2;
        }
        h2 {
            font-size: 1.8em;
            margin-top: 1.5em;
        }
        p {
            margin-bottom: 1.5em;
            text-align: justify;
        }
        .authors {
            font-style: italic;
            color: #7f8c8d;
            margin-bottom: 0.5em;
            text-align: center;
            font-size: 1.1em;
        }
        .authors span {
            display: inline-block;
            margin: 0 10px;
        }
        .paper-info {
            text-align: center;
            color: #2980b9;
            font-size: 1.1em;
            margin-bottom: 1em;
            font-weight: 500;
        }
        .info-container {
            display: flex;
            justify-content: space-between;
            align-items: center;
            margin-bottom: 1em;
        }
        .conference-info {
            background-color: #e0f7fa;
            padding: 8px 15px;
            border-radius: 5px;
            color: #01579b;
            font-size: 0.9em;
            flex: 0 1 auto;
        }
        .conference-info strong {
            font-weight: bold;
        }
        .links {
            display: flex;
            gap: 15px;
            flex: 0 1 auto;
        }
        .links a {
            color: #3498db;
            text-decoration: none;
            display: inline-flex;
            align-items: center;
            font-size: 0.9em;
        }
        .links a:hover {
            text-decoration: underline;
        }
        .links img {
            width: 16px;
            height: 16px;
            margin-right: 5px;
        }
        .abstract {
            background-color: #fdfdfd;
            padding: 3px 20px 20px 20px; /* Reduced top padding */
            border-radius: 5px;
            box-shadow: 0 2px 15px rgba(0,0,0,0.1);
            margin-bottom: 2em;
        }
        .content-section {
            background-color: #fff;
            padding: 3px 20px 20px 20px; /* Reduced top padding */
            border-radius: 5px;
            box-shadow: 0 2px 15px rgba(0,0,0,0.1);
            margin-bottom: 2em;
        }
        .figure {
            background-color: #e0e0e0;
            height: 300px;
            display: flex;
            justify-content: center;
            align-items: center;
            margin: 1em 0;
            font-size: 1.2em;
            color: #7f8c8d;
        }
        .figure-container {
          margin: 2em 0;
          text-align: center;
        }
        .figure-container img {
            max-height: 500px;
            max-width: 100%;
            height: auto;
            /* border: 1px solid #ddd; */
            /* border-radius: 4px; */
            /* box-shadow: 0 2px 4px rgba(0,0,0,0.1); */
        }
        .figure-container2 {
          margin: 2em 0;
          text-align: center;
        }
        .figure-container2 img {
            max-height: 500px;
            max-width: 100%;
            height: auto;
            /* border: 1px solid #ddd; */
            /* border-radius: 4px; */
            /* box-shadow: 0 2px 4px rgba(0,0,0,0.1); */
        }

        .figure-caption {
            margin-top: 0.5em;
            /* font-style: italic; */
            color: #666;
            font-size: 0.9em;
        }
        @media (max-width: 600px) {
            body {
                padding: 10px;
            }
            h1 {
                font-size: 2em;
            }
            .authors span {
                display: block;
                margin: 5px 0;
            }
            .info-container {
                flex-direction: column;
                align-items: stretch;
            }
            .conference-info, .links {
                width: 100%;
                text-align: center;
                margin-bottom: 10px;
            }
            .links {
                justify-content: center;
            }
        }
        .quote {
            border-left: 4px solid #ccc; /* Line on the left */
            padding-left: 16px; /* Indentation */
            margin-left: 16px;
            color: #555; /* Text color */
            font-style: italic; /* Optional: Italicize text */
        }
        .custom-link {
            color: #3498db; /* Custom color for the link */
            text-decoration: none; /* Remove default underline */
        }

        .custom-link:hover {
            color: #2980b9; /* Slightly different color on hover */
            text-decoration: underline; /* Optional: underline on hover */
        }

        .acronym-explanation {
        font-style: italic; /* Italicize text */
        font-size: 0.9em; /* Slightly smaller font */
        color: #555; /* A lighter color for the text */
        text-align: left; /* Align text to the left */
        margin-top: 0px; /* Add a little spacing above */
        }

        .video-container {
            margin: 20px 0;
        }

        .video-controls {
            display: flex;
            justify-content: center;
            gap: 10px;
        }

        .video-controls button {
            padding: 5px 10px; /* Adjust these values to control the padding inside the buttons */
            font-size: 14px;   /* Optionally, reduce font size for smaller buttons */
            cursor: pointer;
        }

        .video-controls .active-button {
            background-color: blue;
            color: white;
        }

        .feature-controls {
            display: flex;
            justify-content: center;
            gap: 12px;
            margin-bottom: 20px;
            flex-wrap: wrap;
        }

        .feature-controls button {
            padding: 10px 20px;
            border: none;
            background-color: #f0f0f0;
            cursor: pointer;
            border-radius: 5px;
            transition: background-color 0.3s ease, color 0.3s ease;
            font-size: 14px;
            white-space: nowrap;
        }
        
        .feature-controls button:hover {
            background-color: #e0e0e0;
        }
        
        .feature-controls button.active {
            background-color: #2563eb;
            color: white;
        }

        #videoPlayer {
            width: 100%;  /* Adjust the width as needed */
            height: 280px;
            object-fit: cover;  /* Ensures the video covers the area */
            overflow: hidden;   /* Hides the cropped portions */
            padding: 0; 
        }

        .cropped-video {
        object-fit: cover; /* Crops the video */
        width: 70%;
        height: auto;
        /* You can further adjust the cropping using these properties */
        object-position: center; /* Center the video inside the container */
        clip-path: inset(0 1% 0 1%); /* top right bottom left */
        }

        .cropped-video2 {
        object-fit: cover; /* Crops the video */
        width: 100%;
        height: auto;
        /* You can further adjust the cropping using these properties */
        object-position: center; /* Center the video inside the container */
        clip-path: inset(0 1% 0 1%); /* top right bottom left */
        }

        .cropped-video3 {
        object-fit: cover; /* Crops the video */
        height: auto;
        /* You can further adjust the cropping using these properties */
        object-position: center; /* Center the video inside the container */
        clip-path: inset(0 1% 0 1%); /* top right bottom left */
        }


        .vertical-cropped-video {
        object-fit: cover; /* Crops the video */
        width: 70%;
        height: auto;
        /* You can further adjust the cropping using these properties */
        object-position: center; /* Center the video inside the container */
        clip-path: inset(5% 0% 5% 0%); /* top right bottom left */
        }


        .new_video {
            border-radius: 15px; /* Adjust the value to control the roundness */
            overflow: hidden; /* Ensures content stays within the rounded borders */
            box-shadow: 0 4px 8px rgba(0, 0, 0, 0.2); /* Optional: Add some shadow for a subtle effect */
        }


        .callout {
            padding: 1em;
            margin: 1em 0;
            border-left: 4px solid #f1c40f; /* Yellow color like a lightbulb */
            background-color: #f9f9f9; /* Light background */
            /* font-size: 1em; */
            display: flex;
            align-items: center;
        }
        .callout a {
            white-space: nowrap; /* Prevents the link from breaking to a new line */
            text-decoration: underline;
        }

        .callout2 {
            background-color: #fdf6e3; /* Soft background color */
            border-left: 4px solid #e2b007; /* Accent border on the left */
            padding: 16px; /* Ample padding for readability */
            margin: 16px 0; /* Vertical spacing */
            border-radius: 8px; /* Rounded corners */
            font-size: 1.1rem; /* Slightly larger font for emphasis */
            color: #333; /* Dark text color for contrast */
            max-width: 700px; /* Optional: Constrain width for readability */
            box-shadow: 0 2px 8px rgba(0, 0, 0, 0.1); /* Soft shadow for depth */
        }

        .callout2 a {
            color: #007acc; /* Distinct color for the link */
            font-weight: bold;
            text-decoration: none;
        }

        .callout2 a:hover {
            text-decoration: underline; /* Underline on hover for link indication */
        }

        .callout2-icon {
            font-weight: bold;
            color: #e2b007;
            font-size: 1.3rem;
            margin-right: 8px;
            vertical-align: middle;
        }


        /* .callout::before {
            content: "💡";
            font-size: 1.5em;
            margin-right: 0.5em;
        } */
        .logo-container {
            display: flex;
            justify-content: center;
            align-items: center;
            gap: 10px; /* Adjust the spacing between logos */
            margin-top: 10px;
        }

        .logo {
            display: inline-block;
        }

        .mit-logo {
            height: 50px;
        }

        .lab-logo {
            height: 40px;
        }

        .copy-link {
            font-size: 0.9em;
            color: #7f8c8d; /* Gray color */
            cursor: pointer;
            margin-left: 10px;
            display: inline-flex;
            align-items: center;
        }
        .copy-link i {
            font-size: 1.2em; /* Adjust size if needed */
            margin-right: 5px;
        }

        .toc-nav {
            position: fixed;
            right: 20px;
            top: 50%;
            transform: translateY(-50%);
            background: rgba(255, 255, 255, 0.95);
            padding: 15px;
            border-radius: 8px;
            box-shadow: 0 2px 10px rgba(0, 0, 0, 0.1);
            max-width: 300px;
            max-height: 80vh;
            overflow-y: auto;
            z-index: 1000;
            font-size: 14px;
        }

        .toc-nav h3 {
            margin: 0 0 10px 0;
            padding-bottom: 8px;
            border-bottom: 1px solid #eee;
            font-size: 16px;
            color: #333;
        }

        .toc-nav ul {
            list-style: none;
            padding: 0;
            margin: 0;
        }

        .toc-nav li {
            margin: 8px 0;
            padding-left: 10px;
            border-left: 2px solid transparent;
            transition: border-color 0.2s ease;
        }

        .toc-nav a {
            color: #666;
            text-decoration: none;
            display: block;
            padding: 4px 8px;
            border-radius: 4px;
            transition: all 0.2s ease;
        }

        .toc-nav a:hover {
            color: #2980b9;
            background: #f5f5f5;
        }

        .toc-nav li.active {
            border-left: 2px solid #2980b9;
        }

        .toc-nav li.active a {
            color: #2980b9;
            font-weight: 500;
        }

        @media (max-width: 1200px) {
            .toc-nav {
                display: none;
            }
        }

        .target-demographic {
            display: inline-block;
            font-size: 0.85em;
            color: #666;
            background-color: #f5f5f5;
            padding: 2px 8px;
            border-radius: 4px;
            margin-left: 8px;
            border-left: 3px solid #2980b9;
        }


        .code-container {
            background-color: #f4f4f4; /* Light gray background */
            border-radius: 8px; /* Rounded corners */
            padding: 12px; /* Slightly smaller padding */
            margin: 8px 0; /* Reduced vertical spacing above and below the container */
            overflow-x: auto; /* Allows horizontal scrolling for long lines */
            font-family: 'Courier New', Courier, monospace; /* Monospaced font */
            font-size: 1rem; /* Font size */
            color: #333; /* Darker text color for better readability */
        }

        .code-container code {
            color: #333; /* Matching the text color */
            font-size: 0.95em; /* Slightly smaller text within code */
            line-height: 1.5; /* Improved readability */
        }

        .footnote-link {
            color: #0066cc;
            cursor: pointer;
            font-size: 0.75em;
            vertical-align: super;
            text-decoration: none;
            margin: 0 2px;
        }

        .footnote-modal {
            display: none;
            position: fixed;
            top: 50%;
            left: 50%;
            transform: translate(-50%, -50%);
            background-color: white;
            padding: 20px;
            border-radius: 8px;
            box-shadow: 0 2px 10px rgba(0,0,0,0.1);
            z-index: 1001;
            max-width: 25%;
            width: auto;
            max-height: 80vh;
            overflow-y: auto;
        }

        /* New styles for grouped footnotes */
        .footnote-group {
            margin-bottom: 15px;
            padding-bottom: 15px;
            border-bottom: 1px solid #eee;
        }

        .footnote-group:last-child {
            border-bottom: none;
            margin-bottom: 0;
            padding-bottom: 0;
        }

        .footnote-number {
            font-weight: bold;
            color: #0066cc;
            margin-right: 10px;
        }

        /* Rest of previous styles */
        .footnote-overlay {
            display: none;
            position: fixed;
            top: 0;
            left: 0;
            right: 0;
            bottom: 0;
            background-color: rgba(0,0,0,0.5);
            z-index: 1000;
        }

        .footnotes-section {
            margin-top: 40px;
            padding-top: 20px;
            border-top: 2px solid #eee;
        }

        .footnote-content {
            display: none;
        }

        .close-modal {
            position: absolute;
            right: 10px;
            top: 10px;
            cursor: pointer;
            font-size: 20px;
            color: #666;
        }

        .video-row {
            display: flex;
            gap: 20px; /* Space between videos */
            justify-content: center; /* Center videos horizontally */
        }

        #activeFeatureImageContainer img {
            width: 85%;
            max-width: 100%; /* Ensures the image doesn't exceed the container's width */
            height: auto;
            object-fit: contain; /* Preserves aspect ratio within the container */
            display: block;
            margin: 0 auto; /* Centers the image */
        }

        .subtitle {
            /* font-weight: normal;
            font-size: 1.25em; */
            color: #555;
            margin-top: 0em;
            margin-bottom: 1em;
        }

        .title {
            /* font-weight: bold; */
            /* font-size: 1.7em; */
            color: #555;
            /* margin-top: 2em; */
            margin-bottom: 0em;
        }


    </style>

<div class="footnote-overlay" id="footnoteOverlay"></div>
<div class="footnote-modal" id="footnoteModal">
    <span class="close-modal">&times;</span>
    <div id="footnoteContent"></div>
</div>

<script>

    function toggleDeveloperSection() {
        var developerSection = document.getElementById("developer-section");
        if (developerSection.style.display === "none") {
            developerSection.style.display = "block";
        } else {
            developerSection.style.display = "none";
        }
    }


    document.addEventListener('DOMContentLoaded', function() {
        // Handle all internal links with hash
        document.querySelectorAll('a[href^="#"]').forEach(anchor => {
            anchor.addEventListener('click', function (e) {
                e.preventDefault();
                const targetId = this.getAttribute('href').slice(1);
                const targetElement = document.getElementById(targetId);
                
                if (targetElement) {
                    targetElement.scrollIntoView({
                        behavior: 'smooth',
                        block: 'start'
                    });
                    
                    // Update URL without jumping
                    window.history.pushState(null, null, `#${targetId}`);
                }
            });
        });

        // Enhance footnote scrolling
        document.querySelectorAll('.footnote-link').forEach(link => {
            link.addEventListener('click', function(e) {
                // First handle the modal display as before
                const footnoteNumber = this.textContent.replace(/[\[\]]/g, '');
                const footnoteContent = document.querySelector(`#footnote-${footnoteNumber}`);
                
                if (footnoteContent) {
                    const modal = document.getElementById('footnoteModal');
                    const content = document.getElementById('footnoteContent');
                    content.innerHTML = `
                        <div class="footnote-group">
                            <span class="footnote-number">[${footnoteNumber}]</span>
                            ${footnoteContent.innerHTML}
                        </div>
                    `;
                    modal.style.display = 'block';
                    document.getElementById('footnoteOverlay').style.display = 'block';
                    
                    // Smooth scroll to the modal
                    modal.scrollIntoView({
                        behavior: 'smooth',
                        block: 'center'
                    });
                }
            });
        });

        // Handle initial load with hash
        if (window.location.hash) {
            const targetElement = document.querySelector(window.location.hash);
            if (targetElement) {
                // Small delay to ensure proper scrolling after page load
                setTimeout(() => {
                    targetElement.scrollIntoView({
                        behavior: 'smooth',
                        block: 'start'
                    });
                }, 100);
            }
        }
    });

    // Update all footnote content elements to have proper IDs
    document.querySelectorAll('.footnote-content').forEach((content, index) => {
        content.id = `footnote-${index + 1}`;
    });


    document.addEventListener('DOMContentLoaded', function() {
        const footnoteElements = document.querySelectorAll('.footnote-content');
        const footnotesList = document.getElementById('footnotesList');
        
        // Process footnotes and identify groups
        let currentGroup = [];
        let allGroups = [];
        let lastNode = null;
        
        footnoteElements.forEach((elem, index) => {
            if (lastNode && areConsecutive(lastNode, elem)) {
                currentGroup.push({ elem, index });
            } else {
                if (currentGroup.length > 0) {
                    allGroups.push([...currentGroup]);
                }
                currentGroup = [{ elem, index }];
            }
            lastNode = elem;
        });
        if (currentGroup.length > 0) {
            allGroups.push(currentGroup);
        }
        
        // Process groups and individual footnotes
        allGroups.forEach(group => {
            if (group.length > 1) {
                // Create grouped footnote
                const startIndex = group[0].index + 1;
                const endIndex = group[group.length - 1].index + 1;
                const link = document.createElement('a');
                link.className = 'footnote-link';
                link.textContent = `[${startIndex}-${endIndex}]`;
                link.onclick = () => showGroupedFootnotes(group);
                
                // Insert single link before first footnote
                group[0].elem.parentNode.insertBefore(link, group[0].elem);
                
                // Add to footnotes list
                group.forEach(({ elem, index }) => {
                    addToFootnotesList(elem, index + 1);
                });
                
                // Remove other footnotes in group
                group.slice(1).forEach(({ elem }) => {
                    if (elem.previousSibling && elem.previousSibling.className === 'footnote-link') {
                        elem.previousSibling.remove();
                    }
                });
            } else {
                // Process individual footnote
                const elem = group[0].elem;
                const index = group[0].index;
                
                const link = document.createElement('a');
                link.className = 'footnote-link';
                link.textContent = `[${index + 1}]`;
                link.onclick = () => showFootnote(elem.innerHTML, index + 1);
                
                elem.parentNode.insertBefore(link, elem);
                addToFootnotesList(elem, index + 1);
            }
        });
    });

    function areConsecutive(node1, node2) {
    // Get the actual text content between two nodes
        let textBetween = '';
        let current = node1;
        
        while (current && current !== node2) {
            // Skip footnote content nodes
            if (!current.classList || !current.classList.contains('footnote-content')) {
                if (current.nodeType === Node.TEXT_NODE) {
                    textBetween += current.textContent;
                } else if (current.nodeType === Node.ELEMENT_NODE) {
                    textBetween += current.textContent;
                }
            }
            current = current.nextSibling;
        }
        
        // Clean up the text (remove whitespace, newlines, etc.)
        textBetween = textBetween.trim();
        
        // Check if there's any significant text between footnotes
        // Also check if they're in the same block-level element
        return textBetween.length < 3 && // Allow only very short separators like commas
            node1.parentElement === node2.parentElement && // Must be in same parent
            !hasBlockElementBetween(node1, node2);
    }

    function hasBlockElementBetween(node1, node2) {
        const blockElements = ['DIV', 'P', 'H1', 'H2', 'H3', 'H4', 'H5', 'H6', 'SECTION'];
        let current = node1;
        
        while (current && current !== node2) {
            if (current.nodeType === Node.ELEMENT_NODE && 
                blockElements.includes(current.tagName)) {
                return true;
            }
            current = current.nextSibling;
        }
        
        return false;
    }

    function showGroupedFootnotes(group) {
        const modal = document.getElementById('footnoteModal');
        const content = document.getElementById('footnoteContent');
        
        content.innerHTML = group.map(({ elem, index }) => `
            <div class="footnote-group">
                <span class="footnote-number">[${index + 1}]</span>
                ${elem.innerHTML}
            </div>
        `).join('');
        
        modal.style.display = 'block';
        document.getElementById('footnoteOverlay').style.display = 'block';
    }

    function showFootnote(content, index) {
        const modal = document.getElementById('footnoteModal');
        const footnoteContent = document.getElementById('footnoteContent');
        
        footnoteContent.innerHTML = `
            <div class="footnote-group">
                <span class="footnote-number">[${index}]</span>
                ${content}
            </div>
        `;
        
        modal.style.display = 'block';
        document.getElementById('footnoteOverlay').style.display = 'block';
    }

    function addToFootnotesList(elem, index) {
        const footnotesList = document.getElementById('footnotesList');
        const footnoteItem = document.createElement('div');
        footnoteItem.className = 'footnote-item';
        footnoteItem.innerHTML = `
            <span class="footnote-number">[${index}]</span>
            ${elem.innerHTML}
        `;
        footnotesList.appendChild(footnoteItem);
    }

    // Close modal handlers
    document.querySelector('.close-modal').onclick = () => {
        document.getElementById('footnoteModal').style.display = 'none';
        document.getElementById('footnoteOverlay').style.display = 'none';
    };

    document.getElementById('footnoteOverlay').onclick = () => {
        document.getElementById('footnoteModal').style.display = 'none';
        document.getElementById('footnoteOverlay').style.display = 'none';
    };

    document.addEventListener('DOMContentLoaded', function() {
        // Create TOC container
        const toc = document.createElement('nav');
        toc.className = 'toc-nav';
        toc.innerHTML = '<h3>Contents</h3><ul></ul>';
        document.body.appendChild(toc);
        
        const tocList = toc.querySelector('ul');
        const sections = document.querySelectorAll('.content-section');
        
        // Ensure all sections have IDs
        sections.forEach((section, index) => {
            if (!section.id) {
                section.id = `section-${index + 1}`;
            }
        });
        
        // Create TOC entries
        sections.forEach(section => {
            const tocTitle = section.getAttribute('data-toc-title');
            if (tocTitle) { // Only create TOC entry if section has a data-toc-title
                const li = document.createElement('li');
                const link = document.createElement('a');
                link.textContent = tocTitle;
                link.href = `#${section.id}`;
                
                link.addEventListener('click', (e) => {
                    e.preventDefault();
                    section.scrollIntoView({ behavior: 'smooth' });
                    window.history.pushState(null, null, link.href);
                });
                
                li.appendChild(link);
                tocList.appendChild(li);
            }
        });

        // Update active section on scroll
        function updateActiveSection() {
            let currentSection = '';
            
            sections.forEach(section => {
                const sectionTop = section.offsetTop;
                const sectionHeight = section.clientHeight;
                const scrollPosition = window.scrollY;
                
                // Consider a section active if we've scrolled past its top
                // and haven't reached the bottom of the section
                if (scrollPosition >= sectionTop - 100 && 
                    scrollPosition < sectionTop + sectionHeight - 100) {
                    currentSection = section.id;
                }
            });
            
            // Update active states in TOC
            tocList.querySelectorAll('li').forEach(li => {
                const link = li.querySelector('a');
                const href = link.getAttribute('href').substring(1); // Remove #
                
                if (href === currentSection) {
                    li.classList.add('active');
                    // Ensure active item is visible in scrollable TOC
                    li.scrollIntoView({ behavior: 'smooth', block: 'nearest' });
                } else {
                    li.classList.remove('active');
                }
            });
        }

        // Initial update
        updateActiveSection();
        
        // Update on scroll with throttling
        let ticking = false;
        document.addEventListener('scroll', () => {
            if (!ticking) {
                window.requestAnimationFrame(() => {
                    updateActiveSection();
                    ticking = false;
                });
                ticking = true;
            }
        });
    });


    document.addEventListener('DOMContentLoaded', function() {
        // Get all h2 elements within content sections
        document.querySelectorAll('.content-section h2').forEach((header, index) => {
            // Ensure each .content-section has a unique id
            const section = header.closest('.content-section');
            if (!section.id) {
                section.id = `section-${index + 1}`;
            }

            // Create a copy link button
            const copyLinkBtn = document.createElement('span');
            copyLinkBtn.classList.add('copy-link');
            copyLinkBtn.innerHTML = '<i class="fas fa-link"></i>';
            copyLinkBtn.setAttribute('title', 'Copy link to section');
            copyLinkBtn.style.marginLeft = '10px';
            copyLinkBtn.style.fontSize = '0.6em';
            copyLinkBtn.style.opacity = '0.7';
            copyLinkBtn.style.cursor = 'pointer';
            copyLinkBtn.style.verticalAlign = 'middle';

            // Add hover effect
            copyLinkBtn.addEventListener('mouseenter', () => {
                copyLinkBtn.style.opacity = '1';
            });
            copyLinkBtn.addEventListener('mouseleave', () => {
                copyLinkBtn.style.opacity = '0.7';
            });

            // Add click handler to copy the URL
            copyLinkBtn.addEventListener('click', (e) => {
                e.preventDefault();
                const url = `${window.location.origin}${window.location.pathname}#${section.id}`;
                
                // Copy to clipboard
                navigator.clipboard.writeText(url)
                    .then(() => {
                        // Show success message
                        const originalTitle = copyLinkBtn.getAttribute('title');
                        copyLinkBtn.setAttribute('title', 'Link copied!');
                        setTimeout(() => {
                            copyLinkBtn.setAttribute('title', originalTitle);
                        }, 2000);
                    })
                    .catch(err => {
                        console.error('Failed to copy:', err);
                    });
            });

            // Add the button next to the header
            header.appendChild(copyLinkBtn);
        });
    });
    var videos = [
        "static/videos/user_study_white_task1 1_25.00.mp4",
        "static/videos/user_study_white_task2 1_25.00.mp4",
        "static/videos/user_study_white_task3 1_25.00.mp4" // Add more video sources if needed
    ];

    // Function to load a video based on index
    function loadVideo(index) {
            const videoPlayer = document.getElementById('videoPlayer');
            
            if (index >= 0 && index < videos.length) {
                // Store current time and playing state
                const wasPlaying = !videoPlayer.paused;
                
                // Update video source
                videoPlayer.src = videos[index];
                
                // After loading new source, play if it was playing before
                videoPlayer.load();
                if (wasPlaying) {
                    videoPlayer.play();
                }

                // Update button styles
                const buttons = document.querySelectorAll('.video-controls button');
                buttons.forEach((button, i) => {
                    if (i === index) {
                        button.classList.add('active-button');
                    } else {
                        button.classList.remove('active-button');
                    }
                });
            }
        }

    // Initialize first video on page load
    window.onload = function() {
        loadVideo(0);
    };

    // Define the features data
    const features = {
        'task-switching': {
            videoSrc: 'static/videos/instant_task_switching 1_26.00.mp4'
        },
        'viewpoint-switching': {
            videoSrc: 'static/videos/egoexo-switch 1_26.00.mp4'
        },
        'env-reset': {
            videoSrc: 'static/videos/random_resets.mp4'
        },
        'mobile-manipulation': {
            videoSrc: 'static/videos/mobile_manip.mp4'
        },
        'multi-robot': {
            imageSrc: 'static/images/multi_robot_6.png' // Add image source here
        },
        'custom-support': {
            // Add video or image source if needed
        }
    };

    function showFeature(featureId) {
        // Update active button state
        document.querySelectorAll('.feature-controls button').forEach(button => {
            button.classList.remove('active');
        });
        event.target.classList.add('active');

        // Get the feature data
        const feature = features[featureId];

        // Update the text content
        for (const [key, value] of Object.entries(features)) {
            const textContainerId = key + '-container';
            const textContainer = document.getElementById(textContainerId);
            if (!textContainer) continue;
            textContainer.style.display = key === featureId ? 'block' : 'none';
        }

        // Update video or image display
        const videoContainer = document.getElementById('activeFeatureVideoContainer');
        const imageContainer = document.getElementById('activeFeatureImageContainer');
        const videoPlayer = document.getElementById('activeFeatureVideoSource');
        const imageElement = document.getElementById('activeFeatureImageSource');

        // Check if the feature has a video
        if (feature.videoSrc) {
            videoPlayer.src = feature.videoSrc;
            videoPlayer.load();
            videoPlayer.play();
            videoContainer.style.display = 'block';
            imageContainer.style.display = 'none';
        } else if (feature.imageSrc) {
            imageElement.src = feature.imageSrc;
            imageContainer.style.display = 'block';
            videoContainer.style.display = 'none';
        } else {
            videoContainer.style.display = 'none';
            imageContainer.style.display = 'none';
        }
    };

</script>

</head>
<body>


    <h1>DexHub and DART: Towards <br>Internet-Scale Robot Data Collection</h1>
    <p class="authors">
        <span>Younghyo Park</span>
        <span>Jagdeep Bhatia</span>
        <span>Lars Ankile</span>
        <span>Pulkit Agrawal</span>
    </p>
    <!-- put MIT logo with fixed width  -->
    <div class="logo-container">
        <img src="static/images/mit_logo_std_rgb_mit-red.png" alt="MIT Logo" class="logo mit-logo">
        <img src="static/images/lab_logo.png" alt="Lab Logo" class="logo lab-logo">
    </div>
    
    <p class="paper-info">ICRA 2025 (Under Review)</p>

    <div class="info-container">
        <div class="conference-info">
            Visit <strong>CoRL 2024 <a href="https://sites.google.com/view/xembodimentworkshop">XE</a>/<a href="https://wcbm-workshop.github.io/">WBCM</a> Workshop</strong> <br>for an in-person demo on Nov 9th, Munich
        </div>
        <div class="links">
            <a href="https://arxiv.org/abs/2407.16186" target="_blank">
                <img src="static/images/papericon.png" alt="Paper icon"> Full Paper
            </a>
            <a href="https://dexhub.ai/" target="_blank">
                <img src="https://api.iconify.design/material-symbols:open-in-new.svg" alt="Portal icon"> DexHub Portal
            </a>
            <a href="https://x.com/younghyo_park/status/1815986042098348163" target="_blank">
              <img src="https://abs.twimg.com/favicons/twitter.ico" alt="Twitter icon"> Twitter
            </a>
          </div>
    </div>

    <!-- division line -->
    <hr style="border: 0; border-top: 1px solid #333; margin: 20px 0;">
    
    <div class="figure-container" style="margin:0%">
        <video width="70%" playsinline autoplay muted>
            <!-- <source src="static/videos/twitter_main_fig.mp4" type="video/mp4"> -->
             <!-- make it auto-loop, with no video control exposed -->
            <source src="static/videos/dart_logo_animated2.mp4" type="video/mp4">
        </video>  
        <caption style="margin:0%">
            <center>
            <b>Try it yourself on Apple Vision Pro!</b> Please see our <a href="#how-to-dart" class="custom-link">getting started guide</a>.<br>
            If you're a researcher working on AI / Robotics, check out our <a href="#call-for-research" class="custom-link">call for research</a>. 
            </center>
        </caption>
        <!-- <img src="static/images/rl_wish.png" alt="What we wish RL could do"> -->
        <!-- <p class="figure-caption">A truly intelligent robot should be able to learn a new task without human supervision, removing the bottleneck to scaling capabilities.</p> -->
    </div>




    <!-- <center>
<iframe width="100%" height="400" src="https://www.youtube.com/embed/wLfRDlVcOWM?si=9d4W9JXMOCx_KN6N" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>        
</center>   -->
    <p> </p>




    <!-- <div class="abstract">
        <h2>Abstract</h2>
    
        <p>The field of robotics has long grappled with a critical challenge: the scarcity of diverse, high-quality data that can be used to train a generalist robot policy. 
            While real-world data collection efforts exist<span class="footnote-content">A. O’Neill, A. Rehman, A. Maddukuri, A. Gupta, A. Padalkar,
                A. Lee, A. Pooley, A. Gupta, A. Mandlekar, A. Jain et al., “Open
                x-embodiment: Robotic learning datasets and rt-x models: Open x-
                embodiment collaboration 0,” in 2024 IEEE International Conference
                on Robotics and Automation (ICRA). IEEE, 2024, pp. 6892–6903.</span><span class="footnote-content">H.-S. Fang, H. Fang, Z. Tang, J. Liu, C. Wang, J. Wang, H. Zhu, and
                C. Lu, “Rh20t: A comprehensive robotic dataset for learning diverse
                skills in one-shot,” in 2024 IEEE International Conference on Robotics
                and Automation (ICRA). IEEE, 2024, pp. 653–660.</span><span class="footnote-content">F. Ebert, Y. Yang, K. Schmeckpeper, B. Bucher, G. Georgakis,
                K. Daniilidis, C. Finn, and S. Levine, “Bridge data: Boosting generalization of robotic skills with cross-domain datasets,” arXiv preprint
                arXiv:2109.13396, 2021.</span><span class="footnote-content">A. Khazatsky, K. Pertsch, S. Nair, A. Balakrishna, S. Dasari, S. Karamcheti, S. Nasiriany, M. K. Srirama, L. Y. Chen, K. Ellis et al., “Droid:
                A large-scale in-the-wild robot manipulation dataset,” arXiv preprint arXiv:2403.12945, 2024.</span>,
            requirements for robot hardware, physical environment setups, and frequent resets significantly impede the scalability needed for modern learning frameworks. To address these limitations, this paper introduces DART, a novel teleoperation platform that reimagines robotic data collection by leveraging cloud-based simulation and augmented reality (AR). Our user studies highlight that DART enables higher data collection throughput and lower physical fatigue compared to real-world teleoperation frameworks. In addition, our policy training experiments using DART-collected datasets demonstrate successful Sim2Real transfer with robust trained behaviors. Most importantly, all data collected through DART is automatically stored in our cloud-hosted database, DexHub, and publicly available to anyone. DexHub also supports easy-to-use data logging APIs that can be inserted into anyone's robot execution script, positioning it as an ever-growing data hub for robot learning.
        </p>
    </div>
     -->

    <!-- <div class="content-section">
    <h2>Key Claims</h2>
      <ol>
        <li>We need to <b>automate</b> the heuristic process of Environment Shaping. </li>
        <li>We need <b>better RL algorithms</b> that don’t require heuristic environment shaping in the first place.</li>
        <li>Developing better RL algorithms starts from <b>benchmarking on unshaped RL environments.</b></li>
    </ol>

    </div> -->


    <!-- <div class="content-section">
        <h2>What is a <b>robot dataset</b>, and why do we need them?</h2>

        <center><b>We want a ChatGPT moment to happen for robotics.</b> </center>
        
        Robotics has seen impressive progress with the advent of
        learning-based control. However, a major bottleneck is the
        lack of diverse and high-quality data for training robust and
        generalizable robot policies. Access to internet-scale robotics
        dataset that continually and rapidly grows with data coming from everywhere in the world will be ideal — just like how
        people easily upload language, images and videos on the
        internet. Despite recent efforts, we are not there yet.
        In this paper, we examine and address many key bottlenecks
        in achieving this dream.
    </div> -->



    <div class="content-section" data-toc-title="How demos are collected today" id="status-quo">
        <h2>How <b>robot datasets</b> are collected today</h2>
        
        <div class="figure-container">
            <video class="new_video" width="100%" playsinline  autoplay loop muted controls>
                <!-- <source src="static/videos/twitter_main_fig.mp4" type="video/mp4"> -->
                 <!-- make it auto-loop, with no video control exposed -->
                <source src="static/videos/typical_real_world_data_collection 1_23.00.mp4" type="video/mp4">
            </video>  
            <!-- <img src="static/images/rl_wish.png" alt="What we wish RL could do"> -->
            <!-- <p class="figure-caption">A truly intelligent robot should be able to learn a new task without human supervision, removing the bottleneck to scaling capabilities.</p> -->
        </div>
    

        <!-- <button class="toggle-button" onclick="toggleContent('step3')">See the details</button>
        <div id="step3" class="toggle-content"> -->

            <!-- Create a list -->
            <ol>
                <li>
                    <b>Buying a robot</b> is <u>expensive</u>, limiting data collection to tech companies or research labs.
                </li>
                <li>
                    <b>Setting up environments</b> requires either (a) constructing a fake setup around the robot in the lab or (b) moving robots to actual sites of interest, both of which are <u>difficult and time-consuming</u>.
                </li>
                <li>
                    <b>Teleoperating the robot</b> suffers from visual occlusions, network delays, and a lack of rich feedback, which can <u>slow down operators</u> and prevent them from performing dynamic or precise tasks.
                </li>
                <li>
                    <b>Resetting the environment</b> after every task completion is <u>time-consuming</u> and physically exhausting. Operators need to context-switch between robot control and environment setup way too frequently. 
                    In addition, ensuring diverse reset states is not as easy as it sounds; human operators often unintentionally repeat certain reset configurations that are easier to solve.
                </li>
                <li>
                    <b>Repetitive jobs</b>, no matter how easy the task is, quickly lead to fatigue and operator burnout. 
                     Unfortunately, the number of required demonstrations scales with task complexity and the extent of required generalization.
                </li>
                <li>
                    <b>Post-processing collected data</b> often happens on a local machine or private cloud. 
                    Different data structures and conventions for data storage <u>make data sharing difficult</u>, which are essential for scaling up.
            </ol>

            <!-- <p>
                <b>Step 1. Buy a robot &nbsp;|&nbsp;</b> 
                Most data collection efforts require <u>expensive</u>, <u>physical robots</u>, limiting data collection to tech companies or research labs.
            </p>

            <p>
                <b>Step 2. Setup an environment &nbsp;|&nbsp;</b> Data collectors must either physically construct a fake environment around the robot or move the robot to an environment of interest. Both are <u>difficult and time-consuming</u>. 
                
            </p>


            <p>
                <b>Step 3. Teleoperate the robot &nbsp;|&nbsp;</b> Visual occlusions, network delays, and a lack of tactile feedback can <u>slow down operators</u> and prevent them from performing dynamic or precise tasks. 
            </p>

            <p>
                <b>Step 4. Reset the environment for every task completion &nbsp;|&nbsp;</b>
                Resetting is <u>time-consuming</u> and both physically and mentally exhausting. Operators need to context-switch between robot control and environment setup, and ensure that resets are diverse.

            </p>
            
            
            <p>
                <b>Step 5. Repeat &nbsp;|&nbsp;</b> Humans struggle to focus when performing repetitive jobs. Unfortunately, The number of required demonstrations scales with task complexity and the extent of required generalization. 
            </p>

            <p>
                <b>Step 6. Post-Process collected data &nbsp;|&nbsp;</b> Often, recorded data is stored on a local machine or private cloud. Different data structures and conventions for data storage <u>make data sharing difficult</u>.
            </p> -->
        <!-- </div> -->
            <center>
                <b>How do we make this process scale to the size of vision/language datasets?</b>            
            </center>


    </div>
    


    <div class="content-section" data-toc-title="DART reimagines data collection" id="dart-intro">
      <h2 style="margin-bottom: 5px;"><b>DART</b> reimagines robot data collection</h2>



      <div class="figure-container"   >
        <video class="new_video" width="100%" playsinline  autoplay loop muted controls>
            <!-- <source src="static/videos/twitter_main_fig.mp4" type="video/mp4"> -->
             <!-- make it auto-loop, with no video control exposed -->
            <source src="static/videos/final_teaser_video 1_26.00.mp4" type="video/mp4">
        </video>  
        <!-- <img src="static/images/rl_wish.png" alt="What we wish RL could do"> -->
        <!-- <p class="figure-caption">A truly intelligent robot should be able to learn a new task without human supervision, removing the bottleneck to scaling capabilities.</p> -->
    </div>


      <p>
        <b>Collect Data Anywhere in Augmented Reality &nbsp;|&nbsp;</b> 
        DART leverages cloud-based simulation and AR to provide a <u>scalable</u>, <u>intuitive</u>, and <u>cost-effective</u> 
        solution for data collection accessible anywhere in the world. This eliminates the need for a physical robot and associated environment setup costs.
      </p>
  
      <p>
        <b>Access Data through <a href="https://dexhub.ai">DexHub</a> &nbsp;|&nbsp;</b> 
        All data collected through DART is <u>automatically logged</u> to the cloud and <u>shared publicly</u> with researchers. 
        We provide both web interface and an easy-to-use Python API for data access. 
      </p>
      
    </div>
      
    <div class="content-section" data-toc-title="How easy is DART to use?"  id="dart-user-study">

      <h2 class="title"><b>DART</b> is intuitive and efficient</h2>
      <h3 class="subtitle" >User study shows higher data collection throughput and lower operator fatigue. </h3>

      <!-- <p>We bet even your mom could use it.</p> -->

      <!-- <div class="callout">
        Can my mom, for instance, use this to collect data for her robot?
        </div> -->


        <p>
            <!-- <b>User Study &nbsp;|&nbsp;&nbsp;</b>   -->
            We recruited twenty robotics novice participants to spend seven minutes collecting demonstrations with the <a href="https://www.rainbow-robotics.com/en_main">Rainbow RB-Y1's</a> and ALOHA's<span class="footnote-content">T. Z. Zhao, V. Kumar, S. Levine, and C. Finn, “Learning fine-grained
                bimanual manipulation with low-cost hardware,” arXiv preprint
                arXiv:2304.13705, 2023.</span>
                default “kinematic double”-based teleoperation interfaces.

                <!-- A total of 20 robotics-novice participants (without any expertise in robotics or prior experience in teleoperating robots) were asked to spend 7 minutes, collecting as many robot demonstrations as possible. 
                For real-world teleoperation, we used each robot's default teleoperation interface; both <a href="https://www.rainbow-robotics.com/en_main">Rainbow RB-Y1</a> and ALOHA<span class="footnote-content">T. Z. Zhao, V. Kumar, S. Levine, and C. Finn, “Learning fine-grained
                    bimanual manipulation with low-cost hardware,” arXiv preprint
                    arXiv:2304.13705, 2023.</span>
                comes with a kinematic double as their default teacher device. -->
        </p>
        <p>
            <!-- <b>Data Throughput Comparison &nbsp;|&nbsp;</b>             Participants were able to collect 2.5x more demonstrations using DART compared to real-world teleoperation. 
            Participants also reported lower physical fatigue and higher task completion satisfaction when using DART. 
            The results suggest that DART is an intuitive and efficient platform for robot data collection. -->

            <!-- <p>Participants collected <b>2.5x more demonstrations</b> using DART than real-world teleoperation and reported lower physical fatigue and higher task completion satisfaction.</p> -->

            <p>Participants collected <b>2.5x more demonstrations on average</b> using DART than real-world teleoperation setups.</p>

            <div class="figure-container">
                <video class="cropped-video3" width="70%" playsinline autoplay muted>
                    <source src="static/videos/animated_user_study_graph.mp4" type="video/mp4">
                    Your browser does not support the video tag.
                </video>
            </div>


            <!-- <div class="video-row">
                <div class="figure-container">
                    <video class="cropped-video2" width="100%" playsinline autoplay muted>
                        <source src="static/videos/animated_user_study_graph.mp4" type="video/mp4">
                        Your browser does not support the video tag.
                    </video>
                </div>
            
                <div class="figure-container">
                    <video class="cropped-video2" width="100%" playsinline autoplay muted>
                        <source src="static/videos/user_study_qualitative_2.mp4" type="video/mp4">
                        Your browser does not support the video tag.
                    </video>
                </div>
            </div> -->


            <!-- <div class="figure-container">
                <video class="cropped-video" width="70%" playsinline  autoplay muted >
                  <source id="videoSource" src="static/videos/animated_user_study_graph.mp4" type="video/mp4">
                  Your browser does not support the video tag.
                </video>
            </div> -->
    
            

            <!-- Many participants attributed this considerable data throughput gap to a) physical fatigue during teleoperation and b) their inability to closely observe local contact interactions, 
            which hindered their ability to perform tasks effectively. This particular attribution becomes evident with controlled ablation studies. -->

        </p>


        <p>
            <!-- <b>Survey Results &nbsp;|&nbsp;</b>      We also asked the participants to provide qualitative feedback on their experience with DART, and existing VR-based simulation teleoperation frameworks.  -->

            Participants attributed this gap to a) <u>reduced physical fatigue</u>, b) <u>better visibility</u> of local contact interactions, and c) <u>easier resetting</u> of the environment. 
            Please see our paper for controlled ablation studies.

            <!-- <div class="figure-container">
                <video class="cropped-video" width="70%" playsinline autoplay muted >
                  <source id="videoSource" src="static/videos/user_study_qualitative_2.mp4" type="video/mp4">
                  Your browser does not support the video tag.
                </video>
            </div> -->

            <div class="figure-container">
                <video class="cropped-video3" width="70%" playsinline autoplay muted>
                    <source src="static/videos/user_study_qualitative_2.mp4" type="video/mp4">
                    Your browser does not support the video tag.
                </video>
            </div>

    
        </p>

        <p>
            In particular, participants spend data-collection time <u>more productively</u> in DART than the real world.

            <!-- <b>How people spend time &nbsp;|&nbsp;</b>  We also analyzed the time spent on different tasks during the data collection process.
            The results show that participants spent significantly less time on environment setup and resetting tasks when using DART compared to real-world teleoperation. -->

            <div class="figure-container">
                <video class="video" width="75%" playsinline autoplay muted>
                  <source id="videoSource" src="static/videos/time_spent_animation-converted.mp4" type="video/mp4">
                  Your browser does not support the video tag.
                </video>
            </div>


        </p>


        <!-- <b>Side-by-side Comparison Videos &nbsp;|&nbsp;</b>  Side-by-side videos comparing the data collection process in DART versus the real-world.  -->
        <p>See side-by-side comparison videos below.</p>

        <div class="video-controls" style="margin-top: 20px;">
            <button onclick="loadVideo(0)">Object Sorting w/ Rainbow RB-Y1</button>
            <button onclick="loadVideo(1)">Precise Insertion w/ Rainbow RB-Y1</button>
            <button onclick="loadVideo(2)">Bolt Nut Sorting w/ ALOHA</button>
        </div>

        <!-- need to show four videos with 2x2 grid -->
        <div class="video-container">
            <video class="vertical-cropped-video" id="videoPlayer" width="100%" playsinline autoplay muted loop>
              <source id="videoSource" src="static/videos/user_study_white_task1 1_25.00.mp4" type="video/mp4">
              Your browser does not support the video tag.
            </video>
        </div>

    </div>

      
    <!-- </div> -->

    
    <div class="content-section" data-toc-title="DART's Features" id="dart-features">
        <h2 class="title">Features of <b>DART</b></h2>
        <h3 class="subtitle">What makes DART so intuitive and efficient?</h3>

        <p>
            DART's features are carefully designed to make data collection more efficient and less fatiguing for operators.
            We focused on making the data collection experience <b>fun and engaging</b>, rather than <b>taxing, repetitive, and boring</b>.
        </p>

        <div class="feature-controls">
            <button class="feature-button active" onclick="showFeature('env-reset')">One-click Reset</button>
            <button class="feature-button" onclick="showFeature('task-switching')">Anti-Boredom</button>
            <button class="feature-button" onclick="showFeature('mobile-manipulation')">Mobile Manipulation</button>
            <button class="feature-button" onclick="showFeature('viewpoint-switching')">Ego/Exo Viewpoint</button>
            <button class="feature-button" onclick="showFeature('multi-robot')">Multi-Robot</button>
            <!-- <button class="feature-button" onclick="showFeature('custom-support')">Custom Environment Support</button> -->
        </div>

        <div id="task-switching-container" style="display: none;">
            <p>
                <b>Anti-boredom Data Collection &nbsp;|&nbsp;</b> Humans are known to lose focus when performing highly repetitive jobs<span class="footnote-content">Häusser, J. A., Schulz‐Hardt, S., Schultze, T., Tomaschek, A., & Mojzisch, A. (2014). Experimental evidence for the effects of task repetitiveness on mental strain and objective work performance. Journal of Organizational Behavior, 35(5), 705-721. <a href="https://onlinelibrary.wiley.com/doi/full/10.1002/job.1920">[link]</a></span>
                , which unfortunately is an essential part of the real-world robot teleoperator's job.
                DART, on the other hand, can be configured to cycle through a wide range of tasks instantly with a click of a button to minimize mental fatigue and boredom coming from endless repetition of a single task; 
                a feature that is not possible in real-world teleoperation setup.
            </p>
        </div>
        <div id="viewpoint-switching-container" style="display: none;">
            <p>
                <b>Ego/Exo-Viewpoint &nbsp;|&nbsp;</b> DART can be easily configured for 1st person or 3rd person viewpoints.
            </p>

        </div>
        <div id="env-reset-container" style="display: block;">
            <p>
                <b>One-Click Environment Reset &nbsp;|&nbsp;</b> Unlike real-world data collection, DART allows users to reset the environment instantly with programatically-ensured randomness, 
                resulting in a (a) reduced mental/physical fatigue for data collectors, (b) increased data collection throughput, and (c) diversity-ensured data.
            </p>
        </div>
        <div id="mobile-manipulation-container" style="display: none;">
            <p>
                <b>Mobile Manipulation &nbsp;|&nbsp;</b> DART tracks every human movements in a global frame; it allows users to teleoperate mobile manipulators and complete tasks that require navigation.
            </p>
        </div>
        <div id="multi-robot-container" style="display: none;">
            <p>
                <b>Multi-Robot Support &nbsp;|&nbsp;</b> DART supports many bimanual robot platforms out-of-the-box. If you want your robot platform to be added, please contact us at <a href="mailto:dexhub@mit.edu">dexhub@mit.edu</a>. 
            
            </p>
        </div>
        <div id="custom-support-container" style="display: none;">
            <p>
                <b>Custom Scenes &nbsp;|&nbsp;</b> Custom scenes can be uploaded through our website.<span class="footnote-content">Custom scene / robot description upload will be available Jan 2025.</span>
            </p>
        </div>


        <div class="figure-container" id="activeFeatureVideoContainer">
            <video class="new_video" id="activeFeatureVideoSource" width="80%" playsinline  autoplay muted loop>
                <source id="videoSource" src="static/videos/random_resets.mp4" type="video/mp4">
                Your browser does not support the video tag.
            </video>
        </div>        
        <div class="figure-container" id="activeFeatureImageContainer" style="display: none; ">
            <img id="activeFeatureImageSource" >
        </div>

      </div>


    <div class="content-section" data-toc-title="DART's design choices" id="dart-design">
      <h2><b>DART's</b> design choices</h2>

      <p>
        <b>Visual Rendering happens locally on the AR device &nbsp;|&nbsp;</b> Compared to existing VR-based teleoperation approaches which send over the entire camera feeds or rendered images over the network
        <span class="footnote-content">A. Iyer, Z. Peng, Y. Dai, I. Guzey, S. Haldar, S. Chintala, and
            L. Pinto, “Open teach: A versatile teleoperation system for robotic
            manipulation,” arXiv preprint arXiv:2403.07870, 2024.</span><span class="footnote-content">X. Cheng, J. Li, S. Yang, G. Yang, and X. Wang, “Open-television:
                teleoperation with immersive active visual feedback,” arXiv preprint
                arXiv:2407.01512, 2024</span>, 
        DART off-loads compute intensive visual rendering processes to an edge AR device, 
        reducing latency without compromising the visual fidelity.
        <div class="video-container">
            <video width="100%" playsinline autoplay muted >
            <source id="videoSource" src="static/videos/dart_structure.mp4" type="video/mp4">
            Your browser does not support the video tag.
            </video>
            </div>
        </p>


        <p>
            <b>Fingertip-based Robot Controls &nbsp;|&nbsp;</b> 
            For an intuitive user experience, DexHub uses built-in hand-tracking rather than relying on external control devices. 
            Parallel jaw grippers are controlled by pinching and releasing fingers. Full SE(3) pose is specified by tracking four hand keypoints, as seen below. 
            <div class="figure-container" >
                <img src="static/images/fingertips.png" width="60%" alt="DART Fingertip Control">
            </div>
            For dexterous hands, full hand keypoints are used to control all joint angles. 


        </p>
    
    

    </div>


    <div class="content-section" data-toc-title="Power of Simulation Demos" id="power-of-simulation">
        <h2><b>Demonstrations in Simulation</b> offers a lot of benefits </h2>
        If you are a robotics researcher who already has access to a physical robot, you might be wondering, 

        <div class="callout">
            Why should I care about collecting data in simulation if I can collect directly in the real world?
        </div>
        We argue that collecting data in simulation offers several benefits over real-world data collection. 

        <p>
            <b>1. Data Augmentation &nbsp;|&nbsp;</b> Simulation allows access to privileged information, such as the robot's, environment's, and objects’ ground-truth state. The ease of randomization and augmentation in simulation enables policy robustness difficult to achieve via real-world data collection. Please see our paper for experiments.

            

        </p>




        <p>
            <b>2. RL Finetuning &nbsp;|&nbsp;</b> One of the powerful benefit of simulation is the possibility of using RL to finetune the policy. 
            Simulation allows for the refinement of human-collected (therefore possibly
            suboptimal) datasets through online reinforcement learning
            using massively parallelizable simulation environments.
            Such refinement can address the potential performance saturation often observed on policies trained only with supervised
            learning<span class="footnote-content">S. Ross and D. Bagnell, “Efficient reductions for imitation learning,”
                in Proceedings of the Thirteenth International Conference on Artificial
                Intelligence and Statistics, ser. Proceedings of Machine Learning
                Research, Y. W. Teh and M. Titterington, Eds., vol. 9. Chia Laguna
                Resort, Sardinia, Italy: PMLR, 13–15 May 2010, pp. 661–668.
                [Online]. <a href="https://proceedings.mlr.press/v9/ross10a.html">[link]</a></span>
                <span class="footnote-content">S. Ross, G. Gordon, and D. Bagnell, “A reduction of imitation learning
                    and structured prediction to no-regret online learning,” in Proceedings
                    of the fourteenth international conference on artificial intelligence and
                    statistics. JMLR Workshop and Conference Proceedings, 2011, pp.
                    627–635.</span>
                <span class="footnote-content">T. Z. Zhao, J. Tompson, D. Driess, P. Florence, S. K. S. Ghasemipour,
                    C. Finn, and A. Wahid, “Aloha unleashed: A simple recipe for robot
                    dexterity,” in 8th Annual Conference on Robot Learning.</span>
                    <span class="footnote-content">L. Ankile, A. Simeonov, I. Shenfeld, M. Torne, and P. Agrawal, “From
imitation to refinement–residual rl for precise visual assembly,” arXiv
preprint arXiv:2407.16677, 2024.
                    </span>.



            <div class="figure-container">
                <video class="cropped-video2" width="100%" playsinline autoplay muted >
                  <source id="videoSource" src="static/videos/bcvsrl 1_25.00.mp4" type="video/mp4">
                  Your browser does not support the video tag.
                </video>
            </div>

        </p>

      </div>
  

      <div class="content-section" data-toc-title="Experience DART yourself!" id="how-to-dart">
        <h2><b>Experience DART yourself!</b></h2>

        <div class="callout2">
            <center>
                Try DART on your own Apple Vision Pro! <br>Sign up <a href="https://dexhub.ai/">here</a> for access to our cloud-hosted physics engine.
            </center>
        </div>

        <div class="figure-container">
            <video class="new_video" width="100%" playsinline autoplay muted controls >
              <source id="videoSource" src="static/videos/dart_tutorial_new2 1_25.00.mp4" type="video/mp4">
              Your browser does not support the video tag.
            </video>
        </div>

        <p>
            Anyone can spin up a <b>cloud-hosted physics engine</b> and start collecting robot data from home. Servers run in the following regions. 
            Join our waitlist to get access.
            <!-- <span class="target-demographic">Target Demographic: <b>General Public</b></span> -->

            <figure>
                <img src="static/images/aws-supported-regions.png" width="100%" alt="DART Fingertip Control">
            </figure>
        </p>

        <div class="collapsible-section" onclick="toggleDeveloperSection()">
            <p style="cursor: pointer; text-decoration: underline;">▶ Are you a developer?</p>
        </div>
        <div id="developer-section" style="display: none; margin-top: 1em;">
            <p>
                Those with the technical know-how can install and run the physics engine on your own machine in a local network.  
                Make sure your machine is powerful enough to run the physics engine in real-time. 
                <!-- <span class="target-demographic">Target Demographic: <b>Robotics Researchers / Engineers</b></span> -->
                <div class="code-container">
                    <pre><code class="language-bash">pip install dart_physics --upgrade   
export DEXHUB_API_TOKEN=...
python -m dart_physics.server</code> </pre>
                </div>
                You can generate and view API tokens through the <a href="https://dexhub.ai">DexHub</a> portal. API tokens are required 
                to log the teleoperated data to the cloud.
            </p>
        </div>
        
      </div>


      <div class="content-section" id="real-world-datasets" data-toc-title="Real-world Demos are still useful">
        <h2><b>Real-world datasets</b> will still be useful</h2>

        <!-- We are not claiming that simulation-collected demos are the only useful source of robot data; 
        In fact, we believe that a few real-world demonstrations will be very useful to fine-tune the policy learned from simulation data, 
        especially for last-mile Sim2Real transfer process.  -->

        <p>
            We hypothesize that the scale and diversity of data required for a robot foundation models
            are most feasibly achieved through simulation. Operators don't have to come in physically into the lab to collect data; 
            they can do it from anywhere in the world. <b>No more safety concerns for part-time data collectors, 
            availability to work-from-home, no robot break downs, etc.</b>
        </p>

        <p>
            However, our work does not aim to replace real-world data collection entirely. 
            Real-world data is paramount for fine-tuning policies learned from simulation data, 
            aiding in last-mile Sim2Real transfer, and serving as a regularizer to prevent over-exploiting simulation and/or deviating too much from the real world.
            
            <div class="figure-container" >
                <img src="static/images/dataset_fraction.png" width="70%" alt="data-fraction">
            </div>
        </p>

        We anticipate that, in the future, robot foundation models will be trained on a mix of approximately 50-70% simulation data and 30-50% real-world data.

        <!-- We still believe, however, that the scale and diversity of robot foundation model's behavior will mostly come from simulation demos, 
        where real-world datasets will mostly serve as a regularizer preventing the policy from over-exploiting the artifacts of simulation and/or deviating too much from the real-world counterparts.  -->


    </div>

  



      <div class="content-section" id="call-for-research" data-toc-title="Call for Research">
        <h2><b>Call for Research</b></h2>

        <p>
            DART and DexHub are still in the early stages of development, with room to grow. We've listed a few ideas we encourage the community to explore.
            <!-- which still lacks a lot of features that can significantly improve the data collection process. -->
        </p>

        <h3><b>Generative Simulation Scene Modeling</b></h3>

        <div class="figure-container">
            <video class="cropped-video3" width="80%" playsinline  autoplay muted>
                <!-- <source src="static/videos/twitter_main_fig.mp4" type="video/mp4"> -->
                 <!-- make it auto-loop, with no video control exposed -->
                <source src="static/videos/generative_simulation.mp4" type="video/mp4">
            </video>
            </div>

            Although teleoperating in simulation removes the need for physical environment setup, 
            it still requires a human to manually design the scene in simulation<div class="footnote-content">Park, Y., Margolis, G. B., & Agrawal, P. Position: Automatic Environment Shaping is the Next Frontier in RL. In Forty-first International Conference on Machine Learning.</div>
            . The process involves the following process: 

            <lo style="margin-top: 3em; margin-bottom: 3em;">
                <li style="padding-left: 20px; margin-bottom: 0.3em;"> 
                    <b>Preparing 3D Assets</b>: Finding the right 3D asset from public database (i.e., Objaverse<div class="footnote-content">Deitke, M., Schwenk, D., Salvador, J., Weihs, L., Michel, O., VanderBilt, E., ... & Farhadi, A. (2023). Objaverse: A universe of annotated 3d objects. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (pp. 13142-13153).
                    </div>), scanning real-world objects, 
                         or manually CAD-ing the objects from scratch. 
                </li>
                <li style="padding-left: 20px; margin-bottom: 0.3em;">
                    <b>Placing Assets in the Scene</b>: Placing the objects in the scene in a semantically meaningful way that resembles the real-world setup, 
                    while maintaining strict non-penetration constraint. 
                </li>
                <li style="padding-left: 20px; margin-bottom: 0.3em;">
                    <b>Reset Strategy Design</b>: Designing a reset strategy that we can use to reset the scene after each task completion.
                </li>
            </lo>

        <p>

            All above, although still easier than setting up a physical environment, are still time-consuming and requires certain level of expertise.
            Thus, a method that can autonomously create <b>manipulatable</b> simulation scenes either from language descriptions or pictures/videos of real-world scenes can further
            bring down the cost of data collection and increase the diversity of the collected data. 
        </p>

        <p>
        Recent advances including GenSim<span class="footnote-content">Wang, L., Ling, Y., Yuan, Z., Shridhar, M., Bao, C., Qin, Y., ... & Wang, X. (2023). Gensim: Generating robotic simulation tasks via large language models. arXiv preprint arXiv:2310.01361.</span>,
        RoboGen<span class="footnote-content">Wang, Y., Xian, Z., Chen, F., Wang, T. H., Wang, Y., Fragkiadaki, K., ... & Gan, C. (2023). Robogen: Towards unleashing infinite data for automated robot learning via generative simulation. arXiv preprint arXiv:2311.01455.
        </span>, Gen2Sim<span class="footnote-content">Katara, Pushkal, Zhou Xian, and Katerina Fragkiadaki. "Gen2sim: Scaling up robot learning in simulation with generative models." 2024 IEEE International Conference on Robotics and Automation (ICRA). IEEE, 2024.
        </span>, RoboCasa<span class="footnote-content">Nasiriany, S., Maddukuri, A., Zhang, L., Parikh, A., Lo, A., Joshi, A., ... & Zhu, Y. (2024). RoboCasa: Large-Scale Simulation of Everyday Tasks for Generalist Robots. arXiv preprint arXiv:2406.02523.</span>, RialTo<span class="footnote-content">Torne, M., Simeonov, A., Li, Z., Chan, A., Chen, T., Gupta, A., & Agrawal, P. (2024). Reconciling reality through simulation: A real-to-sim-to-real approach for robust manipulation. arXiv preprint arXiv:2403.03949.
        </span> have shown promising results, 
        but we still have a long way to go to generate truly diverse and realistic scenes resembling that of real world.
        </p>

        <h3><b>Keypoint Tracking Whole-Body Controllers</b></h3>

        <p>
            DART currently supports controlling (a) fixed-based manipulators in bimanual setup, (b) upper-limb of humanoid platforms, and (c) wheel-based mobile robots from keypoint tracking inputs using Differential IK formulation. 
            The main missing piece, however, is an integration with a whole-body controller that can control a <b>full bipedal robot</b> tracking various set of human keypoints that are available from the device.
            Below are the keypoints that Apple's ARKit provides for a human body. 
        </p>

        <div class="figure-container">
            <img src="static/images/wholebody_future3.png" width="80%" alt="">
        </div>

        
        <p>
            Integrating recent works like HOVER<span class="footnote-content">He, T., Xiao, W., Lin, T., Luo, Z., Xu, Z., Jiang, Z., ... & Zhu, Y. (2024). HOVER: Versatile Neural Whole-Body Controller for Humanoid Robots. arXiv preprint arXiv:2410.21229.</span>
            that can support arbitrary combination of keypoints for whole-body control can be a good starting point.
        </p>
        
        <h3><b>MuJoCo on VisionOS</b> and/or <b>Better Apple RealityKit Physics</b></h3>

        <fig class="figure-container">
            <center>
            <img src="static/images/mujoco_visionos.png" width="70%" alt="">
        </center>
        </fig>

        <p> 
            Although DART's compact-sized message passing allows low-latency communication between the AR device and cloud-hosted physics engine (MuJoCo), 
            it will still be more efficient to run the physics engine on-device. Supporting MuJoCo on VisionOS can be a promising direction to explore, 
            which seems to be possible to run with <a href="https://github.com/liuliu/swift-mujoco">Swift</a>. 
        </p>

        <p>
            One alternative path is to improve the fidelity of internal physics engine that Apple RealityKit supports. Apple's RealityKit supports basic 
            <a href="https://developer.apple.com/documentation/realitykit/physics-collision-detection">collision detection</a> between AR objects for simple physics interactions.
            However, based on our experience, the physics engine is not yet mature enough to support complex robot manipulation tasks; <b>it's simply not designed for roboticists</b>. 
            Features that should be improved include: 
            <lo>
                <li> Robot description support </li>
                <li> Joint kinematics/dynamics support </li>
                <li> Deformable material support </li>
                <li> SDF-based collision detection support for tight tolerance tasks </li> 
                <li> Reliable contact resolution </li>
            </lo>
        </p>




        <h3><b>Data Curation for Robot Learning</b></h3>

        <p> 
            Since the data collected through a crowd-sourced platform will be highly diverse and noisy, 
            a principled way to curate the data to ensure the quality of the data is crucial.
            It is well known that LLM communities have spent significant amount of time and effort to curate the data for training,
            and we believe that a similar effort will be required for robot data collection as well as we enter the regime of larger-scale, crowd-sourced robot data collection.
        </p>


    </div>


    <div class="content-section" id="future-imagined" data-toc-title="Future We Imagine">
        <h2><b>Future we Imagine</b></h2>

        <p>
        We hope this project inspires the robotics community to think about / make progress on a platform that ignites the <b>ecological growth</b> of robot datasets. 
        </p>

        <p>
        We believe that robot dataset with the size of vision and language datasets cannot come from project-level data collection efforts, led by 
        single research labs or tech companies. Instead, it requires a global, community-driven effort, mirroring the internet as a data-source for vision-language model training. 
        </p>

        <div class="figure-container">
            <img src="static/images/future.png" alt="">

        </div>

    





    </div>

    <div class="footnotes-section">
        <div id="footnotesList"></div>
    </div>
    


</body>
</html>